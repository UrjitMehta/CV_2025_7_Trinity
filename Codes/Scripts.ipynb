{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10804236,"sourceType":"datasetVersion","datasetId":6706070}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/Output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:16:12.139012Z","iopub.execute_input":"2025-04-17T06:16:12.139368Z","iopub.status.idle":"2025-04-17T06:16:12.333726Z","shell.execute_reply.started":"2025-04-17T06:16:12.139342Z","shell.execute_reply":"2025-04-17T06:16:12.332475Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# pip install ultralytics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# # do not forgot to restart session ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch\n# import ultralytics\n# from ultralytics.utils.torch_utils import select_device\n\n# print(torch.cuda.is_available())  # Should return True\n# print(torch.cuda.get_device_name(0))  # Should say Tesla T4\n# print(f\"Ultralytics YOLOv8 version: {ultralytics.__version__}\")\n\n# device = select_device('')\n# print(f\"YOLOv8 Selected Device: {device}\")\n\n# print(f\"Ultralytics YOLOv8 version: {ultralytics.__version__}\")\n# print(f\"PyTorch version: {torch.__version__}\")\n# print(f\"CUDA available: {torch.cuda.is_available()}\")\n# print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n# print(f\"YOLOv8 Selected Device: {select_device('')}\")\n\n# print(torch.version.cuda)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YOLO AND Sort","metadata":{}},{"cell_type":"markdown","source":"## Prepare the dataset for YOLOv8","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport yaml\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\n\n# Define the paths\nDATASET_PATH = \"/kaggle/input/cv-multiobject-dectection-dataset\"\nVISDRONE_MOT_TRAIN = os.path.join(DATASET_PATH, \"VisDrone2019-MOT-train/VisDrone2019-MOT-train\")\nVISDRONE_MOT_VAL = os.path.join(DATASET_PATH, \"VisDrone2019-MOT-val/VisDrone2019-MOT-val\")\nVISDRONE_MOT_TEST = os.path.join(DATASET_PATH, \"VisDrone2019-MOT-test-challenge/VisDrone2019-MOT-test-challenge\")\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\n\n# Create output directories\nos.makedirs(OUTPUT_PATH, exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"images\", \"train\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"images\", \"val\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"images\", \"test\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"labels\", \"train\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"labels\", \"val\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"labels\", \"test\"), exist_ok=True)\n\n# VisDrone MOT categories\n# The VisDrone annotation format appears to be:\n# <frame_index>,<target_id>,<bbox_left>,<bbox_top>,<bbox_width>,<bbox_height>,<score>,<object_category>,<truncation>,<occlusion>\n#\n# The object_category is defined as:\n# 0: ignored regions\n# 1: pedestrian\n# 2: people\n# 3: bicycle\n# 4: car\n# 5: van\n# 6: truck\n# 7: tricycle\n# 8: awning-tricycle\n# 9: bus\n# 10: motor\n# 11: others\n\n# Define the class mapping - we'll map VisDrone classes to YOLOv8 format\n# We'll skip class 0 (ignored regions) and class 11 (others)\nclass_mapping = {\n    1: 0,  # pedestrian\n    2: 1,  # people\n    3: 2,  # bicycle\n    4: 3,  # car\n    5: 4,  # van\n    6: 5,  # truck\n    7: 6,  # tricycle\n    8: 7,  # awning-tricycle\n    9: 8,  # bus\n    10: 9,  # motor\n}\n\n# Class names for YAML file\nclass_names = [\n    'pedestrian',\n    'people',\n    'bicycle',\n    'car',\n    'van',\n    'truck',\n    'tricycle',\n    'awning-tricycle',\n    'bus',\n    'motor'\n]\n\ndef convert_annotation(annotation_file, sequence_folder, split_type):\n    \"\"\"\n    Convert VisDrone annotation format to YOLO format\n    \"\"\"\n    sequence_name = os.path.basename(sequence_folder)\n    \n    # Read the annotation file\n    with open(annotation_file, 'r') as f:\n        lines = f.readlines()\n    \n    # Group annotations by frame\n    frame_annotations = {}\n    for line in lines:\n        line = line.strip().split(',')\n        frame_idx = int(line[0])\n        if frame_idx not in frame_annotations:\n            frame_annotations[frame_idx] = []\n        frame_annotations[frame_idx].append(line)\n    \n    # Process each frame\n    for frame_idx, annotations in frame_annotations.items():\n        # Source image path\n        img_file = f\"{frame_idx:07d}.jpg\"\n        img_path = os.path.join(sequence_folder, img_file)\n        \n        if not os.path.exists(img_path):\n            continue\n        \n        # Destination paths\n        dst_img_path = os.path.join(OUTPUT_PATH, \"images\", split_type, f\"{sequence_name}_{img_file}\")\n        dst_label_path = os.path.join(OUTPUT_PATH, \"labels\", split_type, f\"{sequence_name}_{img_file.replace('.jpg', '.txt')}\")\n        \n        # Copy the image\n        shutil.copy(img_path, dst_img_path)\n        \n        # Convert annotations to YOLO format and write to file\n        with open(dst_label_path, 'w') as f:\n            # Get image dimensions\n            # We'll use PIL to get the image dimensions\n            from PIL import Image\n            img = Image.open(img_path)\n            img_width, img_height = img.size\n            \n            for anno in annotations:\n                obj_id = int(anno[1])\n                bbox_left = float(anno[2])\n                bbox_top = float(anno[3])\n                bbox_width = float(anno[4])\n                bbox_height = float(anno[5])\n                obj_class = int(anno[7])\n                \n                # Skip ignored regions and others\n                if obj_class == 0 or obj_class == 11:\n                    continue\n                \n                # Map to YOLO class\n                if obj_class not in class_mapping:\n                    continue\n                yolo_class = class_mapping[obj_class]\n                \n                # Convert bbox to YOLO format (x_center, y_center, width, height) - normalized\n                x_center = (bbox_left + bbox_width / 2) / img_width\n                y_center = (bbox_top + bbox_height / 2) / img_height\n                width = bbox_width / img_width\n                height = bbox_height / img_height\n                \n                # Write to file\n                f.write(f\"{yolo_class} {x_center} {y_center} {width} {height}\\n\")\n\ndef process_dataset():\n    \"\"\"\n    Process the dataset using the existing train/val/test splits\n    \"\"\"\n    # Process training data\n    train_sequences_folder = os.path.join(VISDRONE_MOT_TRAIN, \"sequences\")\n    train_annotations_folder = os.path.join(VISDRONE_MOT_TRAIN, \"annotations\")\n    \n    train_sequences = [f for f in os.listdir(train_sequences_folder) if os.path.isdir(os.path.join(train_sequences_folder, f))]\n    \n    for seq in tqdm(train_sequences, desc=\"Processing train set\"):\n        seq_path = os.path.join(train_sequences_folder, seq)\n        anno_path = os.path.join(train_annotations_folder, f\"{seq}.txt\")\n        if os.path.exists(anno_path):\n            convert_annotation(anno_path, seq_path, \"train\")\n    \n    # Process validation data\n    val_sequences_folder = os.path.join(VISDRONE_MOT_VAL, \"sequences\")\n    val_annotations_folder = os.path.join(VISDRONE_MOT_VAL, \"annotations\")\n    \n    val_sequences = [f for f in os.listdir(val_sequences_folder) if os.path.isdir(os.path.join(val_sequences_folder, f))]\n    \n    for seq in tqdm(val_sequences, desc=\"Processing validation set\"):\n        seq_path = os.path.join(val_sequences_folder, seq)\n        anno_path = os.path.join(val_annotations_folder, f\"{seq}.txt\")\n        if os.path.exists(anno_path):\n            convert_annotation(anno_path, seq_path, \"val\")\n    \n    # Process test data\n    test_sequences_folder = os.path.join(VISDRONE_MOT_TEST, \"sequences\")\n    test_annotations_folder = os.path.join(VISDRONE_MOT_TEST, \"annotations\")\n    \n    test_sequences = [f for f in os.listdir(test_sequences_folder) if os.path.isdir(os.path.join(test_sequences_folder, f))]\n    \n    for seq in tqdm(test_sequences, desc=\"Processing test set\"):\n        seq_path = os.path.join(test_sequences_folder, seq)\n        # For test-challenge, we might not have annotations\n        anno_path = os.path.join(test_annotations_folder, f\"{seq}.txt\")\n        if os.path.exists(anno_path):\n            convert_annotation(anno_path, seq_path, \"test\")\n        else:\n            # If no annotations, just copy images\n            for img_file in os.listdir(seq_path):\n                if img_file.endswith('.jpg'):\n                    src_img_path = os.path.join(seq_path, img_file)\n                    dst_img_path = os.path.join(OUTPUT_PATH, \"images\", \"test\", f\"{seq}_{img_file}\")\n                    shutil.copy(src_img_path, dst_img_path)\n\ndef create_yaml_file():\n    \"\"\"\n    Create YAML configuration file for YOLOv8\n    \"\"\"\n    yaml_content = {\n        'path': OUTPUT_PATH,\n        'train': os.path.join('images', 'train'),\n        'val': os.path.join('images', 'val'),\n        'test': os.path.join('images', 'test'),\n        'nc': len(class_names),  # number of classes\n        'names': class_names\n    }\n    \n    with open(os.path.join(OUTPUT_PATH, 'visdrone.yaml'), 'w') as f:\n        yaml.dump(yaml_content, f, default_flow_style=False)\n\nif __name__ == \"__main__\":\n    print(\"Starting dataset preparation...\")\n    process_dataset()\n    create_yaml_file()\n    \n    # Print some statistics\n    train_images = len(os.listdir(os.path.join(OUTPUT_PATH, \"images\", \"train\")))\n    val_images = len(os.listdir(os.path.join(OUTPUT_PATH, \"images\", \"val\")))\n    test_images = len(os.listdir(os.path.join(OUTPUT_PATH, \"images\", \"test\")))\n    \n    print(f\"Dataset preparation complete!\")\n    print(f\"Train images: {train_images}\")\n    print(f\"Validation images: {val_images}\")\n    print(f\"Test images: {test_images}\")\n    print(f\"YAML configuration file created at: {os.path.join(OUTPUT_PATH, 'visdrone.yaml')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure YOLOv8 model","metadata":{}},{"cell_type":"code","source":"import yaml\nfrom ultralytics import YOLO\nimport os\n\n# Set the paths\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\nMODELS_PATH = \"/kaggle/working/models\"\nYAML_PATH = os.path.join(OUTPUT_PATH, \"visdrone.yaml\")\n\n# Create models directory if it doesn't exist\nos.makedirs(MODELS_PATH, exist_ok=True)\n\ndef select_model_size():\n    \"\"\"\n    Select the appropriate model size based on your requirements\n    - nano: Fastest but least accurate (ideal for resource-constrained environments)\n    - small: Good balance between speed and accuracy\n    - medium: More accurate but slower than small\n    - large: Most accurate but slowest\n    \"\"\"\n    model_sizes = {\n        'n': 'yolov8n',  # Nano\n        's': 'yolov8s',  # Small\n        'm': 'yolov8m',  # Medium\n        'l': 'yolov8l',  # Large\n        'x': 'yolov8x'   # Extra Large\n    }\n    \n    print(\"Select YOLOv8 model size:\")\n    print(\"n - Nano (fastest, least accurate)\")\n    print(\"s - Small (good balance)\")\n    print(\"m - Medium (more accurate, slower)\")\n    print(\"l - Large (most accurate, slowest)\")\n    print(\"x - Extra Large (highest accuracy, very slow)\")\n    \n    choice = input(\"Enter your choice (default: s): \").lower() or 's'\n    if choice not in model_sizes:\n        print(f\"Invalid choice: {choice}. Using Small model.\")\n        choice = 's'\n    \n    model_name = model_sizes[choice]\n    print(f\"Selected model: {model_name}\")\n    return model_name\n\ndef load_config():\n    \"\"\"Load the dataset configuration from YAML file\"\"\"\n    with open(YAML_PATH, 'r') as f:\n        config = yaml.safe_load(f)\n    return config\n\ndef configure_model(model_size):\n    \"\"\"Configure the YOLOv8 model with the selected size\"\"\"\n\n    model_path = os.path.join(MODELS_PATH, f\"{model_size}.pt\")\n    \n    # Check if the model file exists, otherwise download it\n    if not os.path.exists(model_path):\n        print(f\"{model_size}.pt not found. Downloading it now...\")\n        model = YOLO(model_size)  # This will automatically download the model\n        model.export(format=\"torch\")  # Save it in the correct format\n        os.rename(f\"{model_size}.pt\", model_path)  # Move to desired location\n        print(f\"Model downloaded and saved at {model_path}\")\n    else:\n        print(f\"Model found at {model_path}\")\n\n    # Load the pre-trained model\n    model = YOLO(model_path)\n    \n    # Model configuration\n    model_config = {\n        'task': 'detect',\n        'model': model_size,\n        'data': YAML_PATH,\n        'epochs': 10,\n        'patience': 10,\n        'batch': 16,\n        'imgsz': 640,\n        'device': 0,  # Use GPU if available\n        'workers': 8,\n        'project': MODELS_PATH,\n        'name': f'visdrone_{model_size.lower()}',\n    }\n    \n    # Save the configuration\n    config_path = os.path.join(MODELS_PATH, f\"{model_size.lower()}_config.yaml\")\n    with open(config_path, 'w') as f:\n        yaml.dump(model_config, f)\n    \n    print(f\"Model configuration saved to {config_path}\")\n    return model_config\n\ndef main():\n    \"\"\"Main function to configure the YOLOv8 model\"\"\"\n    dataset_config = load_config()\n    print(f\"Dataset configuration loaded from {YAML_PATH}\")\n    print(f\"Number of classes: {dataset_config['nc']}\")\n    print(f\"Classes: {dataset_config['names']}\")\n    \n    model_size = select_model_size()\n    model_config = configure_model(model_size)\n    \n    print(\"\\nModel configuration complete!\")\n    print(\"Next steps:\")\n    print(\"Run the training script\")\n    print(\"Monitor training metrics\")\n    print(\"Evaluate model performance\")\n    \n    return model_config\n\nif __name__ == \"__main__\":\n    model_config = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"# Train YOLOv8 Model\nimport os\nimport yaml\nimport torch\nfrom ultralytics import YOLO\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set the paths\nMODELS_PATH = \"/kaggle/working/models\"\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\nYAML_PATH = os.path.join(OUTPUT_PATH, \"visdrone.yaml\")\n\ndef load_model_config(config_path=None):\n    \"\"\"Load model configuration from file or use default\"\"\"\n    if config_path and os.path.exists(config_path):\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n        return config\n    \n    # Default configuration if no file is provided\n    config_files = [f for f in os.listdir(MODELS_PATH) if f.endswith('_config.yaml')]\n    if config_files:\n        config_path = os.path.join(MODELS_PATH, config_files[0])\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n        return config\n    \n    # Fallback to very basic configuration\n    return {\n        'task': 'detect',\n        'model': 'YOLOv8s.pt',\n        'data': YAML_PATH,\n        'epochs': 10,\n        'patience': 10,\n        'batch': 16,\n        'imgsz': 640,\n        'device': 0,\n        'workers': 8,\n        'project': MODELS_PATH,\n        'name': 'visdrone_default',\n    }\n\ndef check_gpu_availability():\n    \"\"\"Check if GPU is available and print info\"\"\"\n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        print(f\"GPU available! Found {device_count} GPU(s).\")\n        for i in range(device_count):\n            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"Current GPU: {torch.cuda.current_device()}\")\n        print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n        print(f\"Memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n    else:\n        print(\"No GPU available. Training will be slow on CPU.\")\n\ndef custom_hyperparameters():\n    \"\"\"Customize hyperparameters for training\"\"\"\n    hyperparams = {}\n    \n    # Learning rate\n    hyperparams['lr0'] = 0.01  # Initial learning rate\n    hyperparams['lrf'] = 0.01  # Final learning rate ratio\n\n    # Optimizer parameters\n    hyperparams['momentum'] = 0.937\n    hyperparams['weight_decay'] = 0.0005\n    \n    # Augmentation parameters\n    hyperparams['hsv_h'] = 0.015  # HSV-Hue augmentation\n    hyperparams['hsv_s'] = 0.7    # HSV-Saturation augmentation\n    hyperparams['hsv_v'] = 0.4    # HSV-Value augmentation\n    hyperparams['degrees'] = 0.0   # Rotation augmentation\n    hyperparams['translate'] = 0.1  # Translation augmentation\n    hyperparams['scale'] = 0.5     # Scale augmentation\n    hyperparams['fliplr'] = 0.5    # Horizontal flip augmentation\n    hyperparams['mosaic'] = 1.0    # Mosaic augmentation\n    \n    return hyperparams\n\ndef train_model(config, hyperparams=None):\n    \"\"\"Train the YOLOv8 model with the given configuration\"\"\"\n\n    # Ensure we train for only 10 epochs\n    config['epochs'] = 10  \n\n    # Explicitly set device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n    # Load model\n    model = YOLO(config['model'])\n\n    # Train the model\n    results = model.train(\n        data=config['data'],\n        epochs=config['epochs'],\n        patience=config['patience'],\n        batch=config['batch'],\n        imgsz=config['imgsz'],\n        device=device,  # Ensure GPU is used\n        workers=config['workers'],\n        project=config['project'],\n        name=config['name'],\n        exist_ok=True,\n        pretrained=True,\n        **(hyperparams or {})\n    )\n    \n    return model, results\n\n\ndef plot_training_results(results_file):\n    \"\"\"Plot training metrics from results CSV\"\"\"\n    if not os.path.exists(results_file):\n        print(f\"Results file not found: {results_file}\")\n        return\n    \n    # Load results\n    results = pd.read_csv(results_file)\n    \n    # Plot metrics\n    metrics = ['box_loss', 'cls_loss', 'dfl_loss', 'precision', 'recall', 'mAP50', 'mAP50-95']\n    \n    plt.figure(figsize=(20, 14))\n    \n    for i, metric in enumerate(metrics):\n        if metric in results.columns:\n            plt.subplot(3, 3, i+1)\n            plt.plot(results['epoch'], results[metric], 'b-')\n            plt.title(f'Training {metric}')\n            plt.xlabel('Epoch')\n            plt.ylabel(metric)\n            plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(MODELS_PATH, 'training_metrics.png'))\n    plt.show()\n\ndef main():\n    \"\"\"Main function to train the YOLOv8 model\"\"\"\n    # Check GPU availability\n    check_gpu_availability()\n    \n    # Load model configuration\n    config_files = [f for f in os.listdir(MODELS_PATH) if f.endswith('_config.yaml')]\n    if config_files:\n        config_path = os.path.join(MODELS_PATH, config_files[0])\n        print(f\"Loading configuration from {config_path}\")\n        config = load_model_config(config_path)\n    else:\n        print(\"No configuration file found. Using default configuration.\")\n        config = load_model_config()\n    \n    # Customize hyperparameters\n    hyperparams = custom_hyperparameters()\n    \n    # Train the model\n    model, results = train_model(config, hyperparams)\n    \n    # Plot training results\n    results_file = os.path.join(MODELS_PATH, config['name'], 'results.csv')\n    plot_training_results(results_file)\n    \n    print(\"\\nTraining complete!\")\n    print(f\"Model saved at: {os.path.join(MODELS_PATH, config['name'])}\")\n    print(\"Next steps:\")\n    print(\"1. Evaluate model performance\")\n    print(\"2. Run inference on test images\")\n    print(\"3. Export model for deployment\")\n    \n    return model\n\nif __name__ == \"__main__\":\n    model = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Detection","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Load your trained model\nmodel_path = \"/kaggle/working/models/visdrone_yolov8s/weights/best.pt\"\nmodel = YOLO(model_path)\n\n# Path to the test image\nimage_path = \"/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-train/VisDrone2019-MOT-train/sequences/uav0000013_00000_v/0000001.jpg\"\n\n# Run inference on the image\nresults = model.predict(image_path, conf=0.25)\n\n# Plot the results\nplt.figure(figsize=(12, 10))\nimg = cv2.imread(image_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nresult_img = results[0].plot()\nplt.imshow(result_img)\nplt.axis('off')\nplt.title(\"Prediction Results\")\nplt.show()\n\n# Print detection statistics\nprint(f\"Number of detections: {len(results[0].boxes)}\")\nprint(f\"Detected classes: {results[0].names}\")\n\n# Print detailed results for top 5 detections\nfor i, box in enumerate(results[0].boxes[:5]):\n    class_id = int(box.cls)\n    confidence = float(box.conf)\n    x1, y1, x2, y2 = box.xyxy[0].tolist()\n    print(f\"Detection {i+1}: Class: {results[0].names[class_id]}, Confidence: {confidence:.2f}, Coordinates: [{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nfrom glob import glob\nfrom PIL import Image\n\n# Path to the folder containing images\nimage_folder = \"/kaggle/working/yolo_dataset/inference_results/\"\n\n# Get all image file paths\nimage_paths = sorted(glob(os.path.join(image_folder, \"*.jpg\")))  # Change \"*.jpg\" if images have a different extension\n\n# Plot images\nplt.figure(figsize=(15, 15))\ncolumns = 4\nrows = (len(image_paths) + columns - 1) // columns  # Calculate number of rows dynamically\n\nfor i, image_path in enumerate(image_paths, 1):\n    img = Image.open(image_path)\n    plt.subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(os.path.basename(image_path))\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sort","metadata":{}},{"cell_type":"code","source":"pip install torch opencv-python numpy tqdm supervision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install lap","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nfrom ultralytics import YOLO\nfrom tqdm import tqdm\nimport lap  # Linear assignment problem solver\n\nclass KalmanFilter:\n    \"\"\"Simple Kalman Filter implementation for tracking\"\"\"\n    def __init__(self):\n        # State transition matrix\n        self.F = np.array([[1, 0, 0, 0, 1, 0, 0, 0],\n                           [0, 1, 0, 0, 0, 1, 0, 0],\n                           [0, 0, 1, 0, 0, 0, 1, 0],\n                           [0, 0, 0, 1, 0, 0, 0, 1],\n                           [0, 0, 0, 0, 1, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 1, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 1, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 1]])\n        \n        # Measurement matrix\n        self.H = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 1, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 1, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 1, 0, 0, 0, 0]])\n        \n        # Measurement noise\n        self.R = np.eye(4) * 0.1\n        \n        # Process noise\n        self.Q = np.eye(8) * 0.1\n        self.Q[4:, 4:] *= 10\n        \n        # Error covariance\n        self.P = np.eye(8) * 10\n        \n        # State\n        self.x = np.zeros((8, 1))\n        \n    def predict(self):\n        \"\"\"Predict next state\"\"\"\n        self.x = self.F @ self.x\n        self.P = self.F @ self.P @ self.F.T + self.Q\n        return self.x[:4].flatten()\n        \n    def update(self, z):\n        \"\"\"Update state with measurement z\"\"\"\n        z = z.reshape(-1, 1)\n        y = z - self.H @ self.x\n        S = self.H @ self.P @ self.H.T + self.R\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n        self.x = self.x + K @ y\n        I = np.eye(8)\n        self.P = (I - K @ self.H) @ self.P\n        return self.x[:4].flatten()\n        \n    def initiate(self, measurement):\n        \"\"\"Initialize state with first measurement\"\"\"\n        self.x = np.zeros((8, 1))\n        self.x[:4, 0] = measurement\n        return self.x[:4].flatten()\n\n\nclass Track:\n    \"\"\"Track class for DeepSORT\"\"\"\n    count = 0\n    \n    def __init__(self, detection, class_id):\n        self.id = Track.count\n        Track.count += 1\n        \n        self.class_id = class_id\n        self.hits = 1\n        self.age = 1\n        self.time_since_update = 0\n        \n        # Initialize Kalman filter\n        self.kf = KalmanFilter()\n        \n        # Initialize state\n        box = detection[:4]\n        self.mean = self.kf.initiate(box)\n        \n        # Detection confidence\n        self.confidence = detection[4] if len(detection) > 4 else 1.0\n        \n    def predict(self):\n        \"\"\"Predict next state\"\"\"\n        self.mean = self.kf.predict()\n        self.age += 1\n        self.time_since_update += 1\n        return self.mean\n        \n    def update(self, detection):\n        \"\"\"Update track with new detection\"\"\"\n        box = detection[:4]\n        self.mean = self.kf.update(box)\n        self.hits += 1\n        self.time_since_update = 0\n        \n        # Update confidence\n        if len(detection) > 4:\n            self.confidence = detection[4]\n        \n        # Update class_id if provided\n        if len(detection) > 5:\n            self.class_id = detection[5]\n        \n        return self.mean\n        \n    def get_state(self):\n        \"\"\"Get current state\"\"\"\n        return self.mean\n    \n    def is_confirmed(self):\n        \"\"\"Check if track is confirmed\"\"\"\n        return self.hits >= 3\n    \n    def is_deleted(self):\n        \"\"\"Check if track should be deleted\"\"\"\n        return self.time_since_update > 30\n\n\nclass SimpleTracker:\n    \"\"\"Simple implementation of DeepSORT tracker\"\"\"\n    def __init__(self):\n        self.tracks = []\n        self.max_iou_distance = 0.7\n        self.max_age = 30\n        self.min_hits = 3\n        \n    def update(self, detections):\n        \"\"\"Update tracks with new detections\"\"\"\n        # Predict locations of existing tracks\n        for track in self.tracks:\n            track.predict()\n        \n        # Match detections to tracks\n        matches, unmatched_tracks, unmatched_detections = self._match(detections)\n        \n        # Update matched tracks\n        for track_idx, detection_idx in matches:\n            self.tracks[track_idx].update(detections[detection_idx])\n        \n        # Mark unmatched tracks\n        for track_idx in unmatched_tracks:\n            self.tracks[track_idx].time_since_update += 1\n        \n        # Add new tracks\n        for detection_idx in unmatched_detections:\n            detection = detections[detection_idx]\n            class_id = int(detection[5]) if len(detection) > 5 else -1\n            self.tracks.append(Track(detection, class_id))\n        \n        # Remove dead tracks\n        self.tracks = [t for t in self.tracks if not t.is_deleted()]\n        \n        # Get results\n        results = []\n        for track in self.tracks:\n            if track.is_confirmed():\n                box = track.get_state()\n                results.append(np.append(box, [track.id, track.class_id, track.confidence]))\n        \n        return np.array(results) if results else np.empty((0, 7))\n    \n    def _match(self, detections):\n        \"\"\"Match detections to tracks using IoU\"\"\"\n        if len(self.tracks) == 0 or len(detections) == 0:\n            return [], list(range(len(self.tracks))), list(range(len(detections)))\n        \n        # Compute IoU matrix\n        iou_matrix = np.zeros((len(self.tracks), len(detections)))\n        for i, track in enumerate(self.tracks):\n            for j, detection in enumerate(detections):\n                iou_matrix[i, j] = self._iou(track.get_state(), detection[:4])\n        \n        # Convert to cost matrix\n        cost_matrix = 1 - iou_matrix\n        \n        # Solve linear assignment problem\n        row_indices, col_indices = lap.lapjv(cost_matrix, extend_cost=True)[0:2]\n        \n        # Filter matches\n        matches = []\n        unmatched_tracks = []\n        unmatched_detections = []\n        \n        for track_idx, detection_idx in enumerate(col_indices):\n            if detection_idx >= 0:\n                if cost_matrix[track_idx, detection_idx] > self.max_iou_distance:\n                    unmatched_tracks.append(track_idx)\n                    unmatched_detections.append(detection_idx)\n                else:\n                    matches.append((track_idx, detection_idx))\n            else:\n                unmatched_tracks.append(track_idx)\n        \n        # Add unmatched detections\n        for detection_idx in range(len(detections)):\n            if detection_idx not in col_indices:\n                unmatched_detections.append(detection_idx)\n        \n        return matches, unmatched_tracks, unmatched_detections\n    \n    def _iou(self, box1, box2):\n        \"\"\"Calculate IoU between two boxes\"\"\"\n        # Convert to [x1, y1, x2, y2] if necessary\n        box1 = np.array(box1).reshape(-1)\n        box2 = np.array(box2).reshape(-1)\n        \n        # Calculate intersection area\n        x1 = max(box1[0], box2[0])\n        y1 = max(box1[1], box2[1])\n        x2 = min(box1[2], box2[2])\n        y2 = min(box1[3], box2[3])\n        \n        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n        \n        # Calculate union area\n        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n        \n        union = area1 + area2 - intersection\n        \n        # Calculate IoU\n        return intersection / union if union > 0 else 0\n\n\nclass ObjectTracker:\n    \"\"\"Object tracker using YOLOv8 and Simple DeepSORT\"\"\"\n    def __init__(self, model_path):\n        \"\"\"Initialize tracker with YOLOv8 model\"\"\"\n        # Load YOLOv8 model\n        self.model = YOLO(model_path)\n        \n        # Initialize tracker\n        self.tracker = SimpleTracker()\n        \n        # Class names for VisDrone dataset\n        self.class_names = ['pedestrian', 'people', 'bicycle', 'car', 'van', \n                           'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor']\n        \n        # Dictionary to store trajectory points for each object\n        self.trajectories = {}\n        \n        # Dictionary to store unique colors for each track ID\n        self.track_colors = {}\n    \n    def _get_color_for_track(self, track_id):\n        \"\"\"Get a unique color for a track ID\"\"\"\n        if track_id not in self.track_colors:\n            # Generate a random color that's not too dark\n            color = np.random.randint(80, 255, size=3).tolist()\n            self.track_colors[track_id] = color\n        return self.track_colors[track_id]\n    \n    def process_sequence(self, sequence_path, output_path, conf_threshold=0.3):\n        \"\"\"Process image sequence and create tracking video\"\"\"\n        # Get all image files\n        image_files = sorted([f for f in os.listdir(sequence_path) \n                             if f.endswith(('.jpg', '.png', '.jpeg'))])\n        \n        if not image_files:\n            print(f\"No images found in {sequence_path}\")\n            return\n        \n        # Read first image to get dimensions\n        first_image = cv2.imread(os.path.join(sequence_path, image_files[0]))\n        height, width = first_image.shape[:2]\n        \n        # Create output directory\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        \n        # Initialize video writer\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))\n        \n        # Process each frame\n        for img_file in tqdm(image_files, desc=\"Processing frames\"):\n            # Read image\n            img_path = os.path.join(sequence_path, img_file)\n            frame = cv2.imread(img_path)\n            if frame is None:\n                continue\n            \n            # Run YOLOv8 detection\n            results = self.model(frame, conf=conf_threshold, verbose=False)\n            \n            # Prepare detections for tracker\n            detections = []\n            if results[0].boxes.data.shape[0] > 0:\n                # Get detection data: [x1, y1, x2, y2, conf, class_id]\n                boxes = results[0].boxes.data.cpu().numpy()\n                \n                # Format detections for tracker\n                for box in boxes:\n                    x1, y1, x2, y2 = box[:4]\n                    conf = box[4]\n                    class_id = int(box[5]) if len(box) > 5 else 0\n                    detections.append([x1, y1, x2, y2, conf, class_id])\n            \n            # Update tracker\n            if detections:\n                tracks = self.tracker.update(np.array(detections))\n                \n                # First draw trajectories\n                for track in tracks:\n                    # Extract tracking info\n                    x1, y1, x2, y2, track_id, class_id, conf = track\n                    \n                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n                    track_id = int(track_id)\n                    \n                    # Calculate center point of bounding box\n                    center_x = int((x1 + x2) / 2)\n                    center_y = int((y1 + y2) / 2)\n                    \n                    # Get unique color for this track ID\n                    color = self._get_color_for_track(track_id)\n                    \n                    # Add center point to trajectory\n                    if track_id not in self.trajectories:\n                        self.trajectories[track_id] = []\n                    self.trajectories[track_id].append((center_x, center_y))\n                    \n                    # Draw trajectory line\n                    if len(self.trajectories[track_id]) > 1:\n                        points = np.array(self.trajectories[track_id], np.int32)\n                        cv2.polylines(frame, [points], False, color, 2)\n                \n                # Then draw bounding boxes (so they're on top of lines)\n                for track in tracks:\n                    # Extract tracking info\n                    x1, y1, x2, y2, track_id, class_id, conf = track\n                    \n                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n                    class_id = int(class_id)\n                    track_id = int(track_id)\n                    \n                    # Get unique color for this track ID\n                    color = self._get_color_for_track(track_id)\n                    \n                    # Get class name\n                    class_name = self.class_names[class_id] if class_id < len(self.class_names) else 'unknown'\n                    \n                    # Draw bounding box\n                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                    \n                    # # Draw class name and track ID\n                    # label = f\"{class_name}-{track_id}\"\n                    # t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]\n                    # cv2.rectangle(frame, (x1, y1-t_size[1]-10), (x1+t_size[0], y1), color, -1)\n                    # cv2.putText(frame, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n            \n            # Write frame to output video\n            out.write(frame)\n            \n            # Optional: Save each processed frame as an image\n            output_img_dir = os.path.join(os.path.dirname(output_path), \"processed_frames\")\n            os.makedirs(output_img_dir, exist_ok=True)\n            cv2.imwrite(os.path.join(output_img_dir, img_file), frame)\n        \n        # Release video writer\n        out.release()\n        print(f\"Output saved to {output_path}\")\n        print(f\"Processed frames saved to {output_img_dir}\")\n        \n        # Return output path\n        return output_path\n\n    def clear_trajectories(self):\n        \"\"\"Clear all tracked trajectories\"\"\"\n        self.trajectories = {}\n        self.track_colors = {}\n\n\n# Function to process a test sequence\ndef process_test_sequence(model_path, sequence_path):\n    \"\"\"Process a test sequence with tracking\"\"\"\n    # Initialize tracker\n    tracker = ObjectTracker(model_path)\n    \n    # Output path\n    os.makedirs(\"tracking_results\", exist_ok=True)\n    sequence_name = os.path.basename(sequence_path)\n    output_path = f\"tracking_results/{sequence_name}_tracked_shot.mp4\"\n    \n    # Process sequence\n    result_path = tracker.process_sequence(sequence_path, output_path)\n    \n    print(f\"Tracking completed! Video saved to: {result_path}\")\n    return result_path\n\n\n# Main function\ndef main():\n    # Path to your YOLOv8 model\n    model_path = \"yolov8n.pt\"  # Update this with your model path\n    \n    # Direct path to the sequence folder\n    sequence_path = \"/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-test-dev/VisDrone2019-MOT-test-dev/sequences/uav0000009_03358_v/\"\n    \n    # Process the sequence\n    result_path = process_test_sequence(model_path, sequence_path)\n    \n    # Display information about the result\n    print(f\"Tracking results saved to: {result_path}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ResNet50 as a feature extractor for ReID","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nclass VisDroneDataset:\n    def __init__(self, train_path, val_path, test_path):\n        \"\"\"\n        Initialize the VisDrone dataset handler with the paths for train, val, and test\n        Args:\n            train_path: Path to the train dataset\n            val_path: Path to the val dataset\n            test_path: Path to the test dataset\n        \"\"\"\n        self.train_path = train_path\n        self.val_path = val_path\n        self.test_path = test_path\n        \n    def list_sequences(self, split_path):\n        \"\"\"\n        List all sequences in a specific split\n        Args:\n            split_path: Path to the sequences directory (train, val, or test)\n        Returns:\n            List of sequence directories\n        \"\"\"\n        seq_path = os.path.join(split_path, 'sequences')\n        return sorted(glob(os.path.join(seq_path, '*')))\n    \n    def get_annotation_path(self, split_path, sequence_name):\n        \"\"\"\n        Get the path to annotations for a specific sequence\n        Args:\n            split_path: Path to the annotations directory (train, val, or test)\n            sequence_name: Name of the sequence (e.g., 'uav0000013_00000_v')\n        Returns:\n            Path to the annotation file\n        \"\"\"\n        return os.path.join(split_path, 'annotations', f\"{sequence_name}.txt\")\n    \n    def get_frames_path(self, split_path, sequence_name):\n        \"\"\"\n        Get path to all frames for a specific sequence\n        Args:\n            split_path: Path to the sequences directory (train, val, or test)\n            sequence_name: Name of the sequence (e.g., 'uav0000013_00000_v')\n        Returns:\n            Path to the sequence frames directory\n        \"\"\"\n        return os.path.join(split_path, 'sequences', sequence_name)\n\n\n# Example usage\ndataset = VisDroneDataset(\n    train_path='/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-train/VisDrone2019-MOT-train',\n    val_path='/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-val/VisDrone2019-MOT-val',\n    test_path='/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-test-dev/VisDrone2019-MOT-test-dev'\n)\n\n# Get sequences for train, val, or test\ntrain_sequences = dataset.list_sequences(dataset.train_path)\nprint(f\"Found {len(train_sequences)} training sequences\")\n\nif len(train_sequences) > 0:\n    first_sequence = os.path.basename(train_sequences[0])\n    print(f\"First sequence: {first_sequence}\")\n    \n    frames_path = dataset.get_frames_path(dataset.train_path, first_sequence)\n    print(f\"Frames path: {frames_path}\")\n    \n    annotation_path = dataset.get_annotation_path(dataset.train_path, first_sequence)\n    print(f\"Annotation path: {annotation_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:20:44.871185Z","iopub.execute_input":"2025-04-17T06:20:44.871544Z","iopub.status.idle":"2025-04-17T06:20:44.918486Z","shell.execute_reply.started":"2025-04-17T06:20:44.871509Z","shell.execute_reply":"2025-04-17T06:20:44.917662Z"}},"outputs":[{"name":"stdout","text":"Found 56 training sequences\nFirst sequence: uav0000013_00000_v\nFrames path: /kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-train/VisDrone2019-MOT-train/sequences/uav0000013_00000_v\nAnnotation path: /kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-train/VisDrone2019-MOT-train/annotations/uav0000013_00000_v.txt\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"class AnnotationParser:\n    \"\"\"\n    Parse VisDrone MOT annotation format\n    Format: <frame_index>,<target_id>,<bbox_left>,<bbox_top>,<bbox_width>,<bbox_height>,<score>,<object_category>,<truncation>,<occlusion>\n    \"\"\"\n    def __init__(self, annotation_file):\n        self.annotation_file = annotation_file\n        self.annotations = self._load_annotations()\n        \n    def _load_annotations(self):\n        \"\"\"Load annotations from file and organize by frame\"\"\"\n        annotations_by_frame = {}\n        \n        try:\n            with open(self.annotation_file, 'r') as f:\n                for line in f:\n                    parts = line.strip().split(',')\n                    if len(parts) < 10:  # Basic validation\n                        continue\n                        \n                    frame_idx = int(parts[0])\n                    target_id = int(parts[1])\n                    bbox_left = int(float(parts[2]))\n                    bbox_top = int(float(parts[3]))\n                    bbox_width = int(float(parts[4]))\n                    bbox_height = int(float(parts[5]))\n                    score = float(parts[6])\n                    category = int(parts[7])\n                    truncation = int(parts[8])\n                    occlusion = int(parts[9])\n                    \n                    if frame_idx not in annotations_by_frame:\n                        annotations_by_frame[frame_idx] = []\n                        \n                    annotations_by_frame[frame_idx].append({\n                        'target_id': target_id,\n                        'bbox': [bbox_left, bbox_top, bbox_width, bbox_height],\n                        'score': score,\n                        'category': category,\n                        'truncation': truncation,\n                        'occlusion': occlusion\n                    })\n            \n            return annotations_by_frame\n        except FileNotFoundError:\n            print(f\"Warning: Annotation file {self.annotation_file} not found\")\n            return {}\n    \n    def get_frame_annotations(self, frame_idx):\n        \"\"\"Get all annotations for a specific frame\"\"\"\n        return self.annotations.get(frame_idx, [])\n    \n    def get_all_targets(self):\n        \"\"\"Get a set of all unique target IDs in the annotation file\"\"\"\n        targets = set()\n        for frame_idx, annotations in self.annotations.items():\n            for ann in annotations:\n                targets.add(ann['target_id'])\n        return targets\n    \n    def get_target_trajectory(self, target_id):\n        \"\"\"Get the trajectory (bboxes) for a specific target across all frames\"\"\"\n        trajectory = {}\n        for frame_idx, annotations in self.annotations.items():\n            for ann in annotations:\n                if ann['target_id'] == target_id:\n                    trajectory[frame_idx] = ann['bbox']\n        return trajectory\n\n# Test the annotation parser\nif len(train_sequences) > 0:\n    sequence_name = os.path.basename(train_sequences[0])\n    annotation_path = dataset.get_annotation_path(dataset.train_path, sequence_name)\n    parser = AnnotationParser(annotation_path)\n    print(f\"Number of frames with annotations: {len(parser.annotations)}\")\n    print(f\"Number of unique targets: {len(parser.get_all_targets())}\")\n    \n    # Display annotations for first frame if available\n    first_frame = min(parser.annotations.keys()) if parser.annotations else None\n    if first_frame is not None:\n        print(f\"Annotations for frame {first_frame}:\")\n        for ann in parser.get_frame_annotations(first_frame)[:5]:  # Show first 5 annotations\n            print(f\"  Target {ann['target_id']}: bbox={ann['bbox']}, category={ann['category']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:20:50.908862Z","iopub.execute_input":"2025-04-17T06:20:50.909198Z","iopub.status.idle":"2025-04-17T06:20:50.965081Z","shell.execute_reply.started":"2025-04-17T06:20:50.909171Z","shell.execute_reply":"2025-04-17T06:20:50.964401Z"}},"outputs":[{"name":"stdout","text":"Number of frames with annotations: 269\nNumber of unique targets: 60\nAnnotations for frame 1:\n  Target 0: bbox=[593, 43, 174, 190], category=0\n  Target 1: bbox=[489, 307, 41, 62], category=0\n  Target 2: bbox=[424, 543, 35, 88], category=1\n  Target 3: bbox=[416, 530, 36, 85], category=1\n  Target 4: bbox=[531, 641, 87, 114], category=7\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nimport torchvision.models as models\nfrom torch.nn import functional as F\n\nclass ReIDFeatureExtractor:\n    \"\"\"ReID feature extractor using ResNet50\"\"\"\n    def __init__(self, use_gpu=True):\n        self.device = torch.device('cuda') if use_gpu and torch.cuda.is_available() else torch.device('cpu')\n        \n        # Initialize ResNet50 model\n        self.model = models.resnet50(pretrained=True)\n        # Replace the final classifier layer\n        num_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(num_features, 256)  # Reduce to 256 feature dimensions\n        self.model.to(self.device)\n        self.model.eval()  # Set to evaluation mode\n        \n        # Define image transforms\n        self.transform = T.Compose([\n            T.ToPILImage(),\n            T.Resize((256, 128)),  # Standard size for person ReID\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    def extract_features(self, image_patches):\n        \"\"\"\n        Extract features from image patches\n        Args:\n            image_patches: List of cropped image patches (numpy arrays)\n        Returns:\n            Tensor of features (N x 256)\n        \"\"\"\n        if not image_patches:\n            return None\n            \n        # Apply transforms to all patches\n        batch = torch.stack([self.transform(patch) for patch in image_patches])\n        batch = batch.to(self.device)\n        \n        # Extract features\n        with torch.no_grad():\n            features = self.model(batch)\n            # L2 normalize features\n            features = F.normalize(features, p=2, dim=1)\n            \n        return features.cpu()\n    \n    def compute_distance(self, feature1, feature2):\n        \"\"\"\n        Compute distance between two feature vectors using cosine similarity\n        Args:\n            feature1, feature2: Feature vectors (torch tensors of any dimension)\n        Returns:\n            Distance score (lower means more similar)\n        \"\"\"\n        # Flatten and normalize both features if needed\n        if feature1.dim() > 1:\n            feature1 = feature1.view(-1)\n        if feature2.dim() > 1:\n            feature2 = feature2.view(-1)\n            \n        # Normalize\n        feature1 = F.normalize(feature1, p=2, dim=0)\n        feature2 = F.normalize(feature2, p=2, dim=0)\n        \n        # Compute cosine similarity directly using dot product of normalized vectors\n        similarity = torch.dot(feature1, feature2).item()\n        \n        # Convert to distance (0-2 range, where 0 means identical)\n        distance = 1 - similarity\n        return distance\n\n# Function to extract patches from frames based on detections\ndef extract_patches(frame, detections):\n    \"\"\"\n    Extract image patches from a frame based on detection bounding boxes\n    Args:\n        frame: OpenCV image\n        detections: List of detection dictionaries with 'bbox' key\n    Returns:\n        List of cropped patches and corresponding detection indices\n    \"\"\"\n    patches = []\n    indices = []\n    \n    for i, det in enumerate(detections):\n        bbox = det['bbox']  # [left, top, width, height]\n        \n        # Handle out-of-bounds coordinates\n        x1 = max(0, bbox[0])\n        y1 = max(0, bbox[1])\n        x2 = min(frame.shape[1], bbox[0] + bbox[2])\n        y2 = min(frame.shape[0], bbox[1] + bbox[3])\n        \n        # Skip if bbox is invalid\n        if x2 <= x1 or y2 <= y1:\n            continue\n            \n        patch = frame[y1:y2, x1:x2]\n        \n        # Skip very small patches\n        if patch.shape[0] < 10 or patch.shape[1] < 10:\n            continue\n            \n        patches.append(patch)\n        indices.append(i)\n    \n    return patches, indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:20:54.384401Z","iopub.execute_input":"2025-04-17T06:20:54.384706Z","iopub.status.idle":"2025-04-17T06:20:54.397406Z","shell.execute_reply.started":"2025-04-17T06:20:54.384682Z","shell.execute_reply":"2025-04-17T06:20:54.396353Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"class ReIDTracker:\n    \"\"\"\n    A simple ReID-based tracker that uses feature similarity\n    \"\"\"\n    def __init__(self, reid_model, distance_threshold=0.5, max_age=30):\n        self.reid_model = reid_model\n        self.distance_threshold = distance_threshold  # Max distance for association\n        self.max_age = max_age  # Max frames to keep a track without matching\n        self.tracks = {}  # Dictionary of track_id -> track info\n        self.next_id = 1  # Next available track ID\n        \n    def update(self, frame, detections):\n        \"\"\"\n        Update tracker with new detections\n        Args:\n            frame: Current frame (OpenCV image)\n            detections: List of detection dictionaries with 'bbox' key\n        Returns:\n            Updated list of detections with 'target_id' key\n        \"\"\"\n        # Extract patches and features for current detections\n        patches, valid_indices = extract_patches(frame, detections)\n        \n        if not patches:\n            # Increment age of all tracks since no detections were matched\n            for track_id in self.tracks:\n                self.tracks[track_id]['age'] += 1\n                \n            # Remove old tracks\n            self.tracks = {k: v for k, v in self.tracks.items() if v['age'] <= self.max_age}\n            return detections  # Return original detections (empty or invalid)\n        \n        # Extract features\n        features = self.reid_model.extract_features(patches)\n        \n        # Match detections to existing tracks\n        matched_track_indices = []\n        matched_detection_indices = []\n        \n        # For each detection\n        for i, det_idx in enumerate(valid_indices):\n            det_feature = features[i].unsqueeze(0)\n            best_track_id = None\n            best_distance = float('inf')\n            \n            # Find best matching track\n            for track_id, track_info in self.tracks.items():\n                if track_id in matched_track_indices:\n                    continue  # Skip already matched tracks\n                    \n                # Calculate distance between current detection and track\n                distance = self.reid_model.compute_distance(det_feature, track_info['feature'])\n                \n                if distance < best_distance and distance < self.distance_threshold:\n                    best_distance = distance\n                    best_track_id = track_id\n            \n            if best_track_id is not None:\n                # Match found, update track\n                matched_track_indices.append(best_track_id)\n                matched_detection_indices.append(det_idx)\n                \n                # Update track information\n                self.tracks[best_track_id]['bbox'] = detections[det_idx]['bbox']\n                # Feature evolution: update feature by weighted average of old and new\n                alpha = 0.9  # Weight for historical feature (controls feature evolution rate)\n                updated_feature = alpha * self.tracks[best_track_id]['feature'] + (1 - alpha) * det_feature\n                updated_feature = F.normalize(updated_feature, p=2, dim=1)  # Re-normalize\n                \n                self.tracks[best_track_id]['feature'] = updated_feature\n                self.tracks[best_track_id]['age'] = 0  # Reset age\n                \n                # Update detection with track ID\n                detections[det_idx]['target_id'] = best_track_id\n        \n        # Create new tracks for unmatched detections\n        for i, det_idx in enumerate(valid_indices):\n            if det_idx not in matched_detection_indices:\n                track_id = self.next_id\n                self.next_id += 1\n                \n                self.tracks[track_id] = {\n                    'feature': features[i].unsqueeze(0),\n                    'bbox': detections[det_idx]['bbox'],\n                    'age': 0\n                }\n                \n                # Update detection with new track ID\n                detections[det_idx]['target_id'] = track_id\n        \n        # Increment age for unmatched tracks\n        for track_id in self.tracks:\n            if track_id not in matched_track_indices:\n                self.tracks[track_id]['age'] += 1\n        \n        # Remove old tracks\n        self.tracks = {k: v for k, v in self.tracks.items() if v['age'] <= self.max_age}\n        \n        return detections","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:21:01.538362Z","iopub.execute_input":"2025-04-17T06:21:01.538691Z","iopub.status.idle":"2025-04-17T06:21:01.548920Z","shell.execute_reply.started":"2025-04-17T06:21:01.538661Z","shell.execute_reply":"2025-04-17T06:21:01.548031Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"import tqdm\nimport time\n\ndef run_reid_tracker(sequence_path, annotation_path, visualize=True, save_video=False, output_path=None):\n    \"\"\"\n    Run the ReID tracker on a sequence\n    Args:\n        sequence_path: Path to the sequence frames\n        annotation_path: Path to the ground truth annotations\n        visualize: Whether to visualize tracking results\n        save_video: Whether to save results as a video\n        output_path: Path to save the output video\n    Returns:\n        Tracking results and metrics\n    \"\"\"\n    # Initialize components\n    frame_reader = FrameReader(sequence_path)\n    annotation_parser = AnnotationParser(annotation_path)\n    \n    # Initialize ReID model and tracker\n    reid_model = ReIDFeatureExtractor(use_gpu=True)\n    tracker = ReIDTracker(reid_model, distance_threshold=0.3, max_age=30)\n    \n    # Results storage\n    tracking_results = {}\n    color_map = {}\n    \n    # Video writer setup\n    video_writer = None\n    if save_video and output_path:\n        # Get first frame to determine video dimensions\n        first_frame = frame_reader.get_frame(1)\n        if first_frame is not None:\n            h, w = first_frame.shape[:2]\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            video_writer = cv2.VideoWriter(output_path, fourcc, 15, (w, h))\n    \n    # Process frames\n    total_frames = frame_reader.get_total_frames()\n    \n    for frame_idx in tqdm.tqdm(range(1, total_frames + 1)):\n        # Read frame\n        frame = frame_reader.get_frame(frame_idx)\n        if frame is None:\n            continue\n        \n        # Get ground truth detections for this frame\n        gt_detections = annotation_parser.get_frame_annotations(frame_idx)\n        \n        # In a real application, we would run object detection here\n        # For this example, we'll use ground truth as detections but remove the target_id\n        # This simulates running a detector but not knowing the target IDs\n        detections = []\n        for det in gt_detections:\n            # Create a copy without target_id\n            detection = {\n                'bbox': det['bbox'],\n                'score': det['score'],\n                'category': det['category']\n            }\n            detections.append(detection)\n        \n        # Track objects\n        tracked_detections = tracker.update(frame, detections)\n        \n        # Save results\n        tracking_results[frame_idx] = tracked_detections\n        \n        # Visualization\n        if visualize or save_video:\n            vis_frame, color_map = visualize_detections(frame, tracked_detections, color_map)\n            \n            if visualize:\n                # Convert to RGB for matplotlib\n                rgb_frame = cv2.cvtColor(vis_frame, cv2.COLOR_BGR2RGB)\n                \n                plt.figure(figsize=(12, 8))\n                plt.imshow(rgb_frame)\n                plt.title(f\"Frame {frame_idx}\")\n                plt.axis('off')\n                plt.show()\n                plt.close()\n                \n                # Add a small delay\n                time.sleep(0.1)\n            \n            if save_video and video_writer:\n                video_writer.write(vis_frame)\n    \n    # Release video writer\n    if video_writer:\n        video_writer.release()\n    \n    return tracking_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:21:17.816385Z","iopub.execute_input":"2025-04-17T06:21:17.816706Z","iopub.status.idle":"2025-04-17T06:21:17.825077Z","shell.execute_reply.started":"2025-04-17T06:21:17.816679Z","shell.execute_reply":"2025-04-17T06:21:17.824135Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# Run tracker on a small test sequence\nif len(train_sequences) > 0:\n    sequence_name = os.path.basename(train_sequences[0])\n    sequence_path = dataset.get_frames_path(dataset.train_path, sequence_name)\n    annotation_path = dataset.get_annotation_path(dataset.train_path, sequence_name)\n    \n    # Set the output path for saving video\n    output_video_path = \"/kaggle/working/tracking_results_res.mp4\"\n    \n    # Run the tracker on the first 100 frames to test\n    frame_reader = FrameReader(sequence_path)\n    total_frames = min(100, frame_reader.get_total_frames())\n    \n    print(f\"Running tracker on sequence {sequence_name} ({total_frames} frames)\")\n    \n    # To save time during development, you might want to disable visualization\n    results = run_reid_tracker(\n        sequence_path, \n        annotation_path, \n        visualize=False,\n        save_video=True, \n        output_path=output_video_path\n    )\n    \n    print(f\"Tracking complete. Results saved to {output_video_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:21:19.748144Z","iopub.execute_input":"2025-04-17T06:21:19.748566Z","iopub.status.idle":"2025-04-17T06:21:56.749750Z","shell.execute_reply.started":"2025-04-17T06:21:19.748521Z","shell.execute_reply":"2025-04-17T06:21:56.748746Z"}},"outputs":[{"name":"stdout","text":"Running tracker on sequence uav0000013_00000_v (100 frames)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n100%|██████████| 269/269 [00:36<00:00,  7.38it/s]","output_type":"stream"},{"name":"stdout","text":"Tracking complete. Results saved to /kaggle/working/tracking_results_res.mp4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"## Deepsort","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport urllib.request\nimport shutil\nimport subprocess\n\n# Define root directory\ncurrent_dir = os.getcwd()\nROOT_DIR = current_dir\nOUTPUT_DIR = os.path.join(ROOT_DIR, 'outputs')\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Print the current working directory for reference\nprint(f\"Current working directory: {current_dir}\")\n\ndef list_directory(path, indent=0):\n    \"\"\"Print the directory structure for debugging\"\"\"\n    if not os.path.exists(path):\n        print(' ' * indent + f\"Path does not exist: {path}\")\n        return\n        \n    print(' ' * indent + f\"Directory: {os.path.basename(path)}\")\n    try:\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isdir(item_path):\n                list_directory(item_path, indent + 2)\n            else:\n                print(' ' * (indent + 2) + f\"File: {item}\")\n    except PermissionError:\n        print(' ' * (indent + 2) + \"Permission denied\")\n    except FileNotFoundError:\n        print(' ' * (indent + 2) + \"Directory not found\")\n\ndef download_deepsort_pytorch():\n    \"\"\"Download and set up DeepSORT PyTorch version\"\"\"\n    \n    print(\"Setting up DeepSORT PyTorch version...\")\n    \n    # Clone DeepSORT PyTorch repository if not exists\n    if not os.path.exists(os.path.join(ROOT_DIR, 'deep_sort')):\n        print(\"Cloning the DeepSORT PyTorch repository...\")\n        subprocess.run([\"git\", \"clone\", \"https://github.com/ZQPei/deep_sort.git\", f\"{ROOT_DIR}/deep_sort\"])\n    else:\n        print(\"DeepSORT directory already exists. Skipping clone.\")\n    \n    # Create config directory if it doesn't exist\n    config_dir = os.path.join(ROOT_DIR, 'deep_sort/configs')\n    os.makedirs(config_dir, exist_ok=True)\n    \n    # Create default config file if it doesn't exist\n    config_file = os.path.join(config_dir, 'deep_sort.yaml')\n    if not os.path.exists(config_file):\n        with open(config_file, 'w') as f:\n            f.write(\"\"\"\nDEEPSORT:\n  REID_CKPT: \"deep_sort/deep_sort/deep/checkpoint/ckpt.t7\"\n  MAX_DIST: 0.2\n  MIN_CONFIDENCE: 0.3\n  NMS_MAX_OVERLAP: 0.5\n  MAX_IOU_DISTANCE: 0.7\n  MAX_AGE: 70\n  N_INIT: 3\n  NN_BUDGET: 100\n            \"\"\")\n    \n    # Download DeepSORT model weights (PyTorch version)\n    weights_dir = os.path.join(ROOT_DIR, 'deep_sort/deep_sort/deep/checkpoint')\n    os.makedirs(weights_dir, exist_ok=True)\n    weights_file = os.path.join(weights_dir, 'ckpt.t7')\n    \n    if not os.path.exists(weights_file) or os.path.getsize(weights_file) < 1000:\n        print(\"Downloading DeepSORT PyTorch model weights...\")\n        # URL for the PyTorch version weights\n        url = \"https://github.com/ZQPei/deep_sort/releases/download/v1.0.2/ckpt.t7\"\n        \n        try:\n            print(f\"Downloading from: {url}\")\n            # Try curl first for reliable downloads\n            try:\n                subprocess.run([\"curl\", \"-L\", url, \"-o\", weights_file], check=True)\n            except (subprocess.SubprocessError, FileNotFoundError):\n                # Fall back to urllib if curl fails\n                urllib.request.urlretrieve(url, weights_file)\n            \n            # Check if download was successful\n            if os.path.exists(weights_file) and os.path.getsize(weights_file) > 1000:\n                print(f\"PyTorch model downloaded successfully to {weights_file}\")\n                print(f\"Model file size: {os.path.getsize(weights_file)} bytes\")\n            else:\n                print(\"Download failed or file is too small\")\n                # Try alternative download method\n                print(\"Trying alternative download method...\")\n                \n                # Using wget if available\n                try:\n                    subprocess.run([\"wget\", url, \"-O\", weights_file], check=True)\n                except (subprocess.SubprocessError, FileNotFoundError):\n                    # Direct file access as last resort\n                    print(\"Using a direct Python download method...\")\n                    with urllib.request.urlopen(url) as response, open(weights_file, 'wb') as out_file:\n                        shutil.copyfileobj(response, out_file)\n        except Exception as e:\n            print(f\"Download error: {e}\")\n            print(\"Creating a placeholder model file for testing purposes only.\")\n            with open(weights_file, 'wb') as f:\n                f.write(b'PLACEHOLDER MODEL FILE FOR TESTING ONLY')\n    else:\n        print(f\"PyTorch model weights already exist at {weights_file}\")\n    \n    print(\"DeepSORT PyTorch setup complete!\")\n\ndef setup_tensorflow_model():\n    \"\"\"Set up the TensorFlow model for DeepSORT\"\"\"\n    \n    print(\"\\nSetting up TensorFlow model for DeepSORT...\")\n    \n    # Create proper directory for the model file\n    model_dir = os.path.join(ROOT_DIR, 'deep_sort/resources/networks')\n    os.makedirs(model_dir, exist_ok=True)\n    model_path = os.path.join(model_dir, 'mars-small128.pb')\n    \n    # Remove the existing empty file if it exists\n    if os.path.exists(model_path) and os.path.getsize(model_path) == 0:\n        print(f\"Removing empty model file: {model_path}\")\n        os.remove(model_path)\n    \n    # List of alternative model sources to try\n    model_urls = [\n        \"https://github.com/theAIGuysCode/yolov4-deepsort/raw/master/model_data/mars-small128.pb\",\n        \"https://github.com/nwojke/cosine_metric_learning/releases/download/v1.0/mars-small128.pb\",\n        \"https://github.com/ZQPei/deep_sort/raw/master/deep_sort/deep/checkpoint/ckpt.t7\"\n    ]\n    \n    # Try downloading from multiple sources\n    model_downloaded = False\n    if not os.path.exists(model_path) or os.path.getsize(model_path) < 1000:\n        for url in model_urls:\n            print(f\"Attempting to download TensorFlow model from: {url}\")\n            try:\n                # Use curl for more reliable downloads\n                try:\n                    subprocess.run([\"curl\", \"-L\", url, \"-o\", model_path], check=True)\n                except (subprocess.SubprocessError, FileNotFoundError):\n                    # Fall back to urllib if curl fails\n                    urllib.request.urlretrieve(url, model_path)\n                \n                # Check if download was successful\n                if os.path.exists(model_path) and os.path.getsize(model_path) > 1000:\n                    print(f\"TensorFlow model downloaded successfully to {model_path}\")\n                    print(f\"Model file size: {os.path.getsize(model_path)} bytes\")\n                    model_downloaded = True\n                    break\n                else:\n                    print(\"Download failed or file is too small\")\n                    # Remove the file if it's too small\n                    if os.path.exists(model_path):\n                        os.remove(model_path)\n            except Exception as e:\n                print(f\"Download error: {e}\")\n    else:\n        print(f\"TensorFlow model file already exists at {model_path} with proper size\")\n        model_downloaded = True\n    \n    if not model_downloaded:\n        print(\"All download attempts failed. Creating a simple model replacement.\")\n        # If all downloads fail, we create a placeholder model file for testing\n        print(\"Creating a placeholder model file for testing purposes only\")\n        with open(model_path, 'wb') as f:\n            f.write(b'PLACEHOLDER MODEL FILE FOR TESTING ONLY')\n        print(\"Placeholder model created. Note: This won't work for actual tracking.\")\n    \n    print(\"TensorFlow model setup complete!\")\n\ndef setup_and_test_imports():\n    \"\"\"Set up sys.path and test imports\"\"\"\n    \n    print(\"\\nSetting up path for imports...\")\n    \n    # Add the correct paths to system path\n    sys.path.append(os.path.join(ROOT_DIR, 'deep_sort'))\n    sys.path.append(os.path.join(ROOT_DIR, 'deep_sort/tools'))\n    \n    print(\"Testing DeepSORT imports...\")\n    \n    try:\n        # Try importing with correct paths\n        from deep_sort import nn_matching\n        from deep_sort.detection import Detection\n        from deep_sort.tracker import Tracker\n        from deep_sort import preprocessing\n        import tools.generate_detections as gdet\n        print(\"Successfully imported Deep SORT modules using direct path\")\n        \n        # Also try importing PyTorch specific modules\n        try:\n            from deep_sort.utils.parser import get_config\n            from deep_sort.deep_sort import DeepSort\n            print(\"Successfully imported Deep SORT PyTorch modules\")\n        except ImportError as e:\n            print(f\"PyTorch DeepSORT modules import error: {e}\")\n            print(\"PyTorch imports may need manual configuration.\")\n            \n    except ImportError as e:\n        print(f\"Import error with direct path: {e}\")\n        try:\n            # Second attempt with different import structure\n            import nn_matching\n            from detection import Detection\n            from tracker import Tracker\n            import preprocessing\n            from tools.generate_detections import create_box_encoder\n            print(\"Successfully imported Deep SORT modules using alternative path\")\n        except ImportError as e2:\n            print(f\"Second import attempt failed: {e2}\")\n            # Let's try one more approach\n            try:\n                import importlib.util\n                \n                # Manually import each module using importlib\n                def import_from_file(module_name, file_path):\n                    spec = importlib.util.spec_from_file_location(module_name, file_path)\n                    if spec is None:\n                        print(f\"Could not find file: {file_path}\")\n                        return None\n                    module = importlib.util.module_from_spec(spec)\n                    spec.loader.exec_module(module)\n                    sys.modules[module_name] = module\n                    return module\n                \n                # Import key modules\n                nn_matching = import_from_file(\"nn_matching\", os.path.join(ROOT_DIR, \"deep_sort/deep_sort/nn_matching.py\"))\n                detection = import_from_file(\"detection\", os.path.join(ROOT_DIR, \"deep_sort/deep_sort/detection.py\"))\n                tracker = import_from_file(\"tracker\", os.path.join(ROOT_DIR, \"deep_sort/deep_sort/tracker.py\"))\n                preprocessing = import_from_file(\"preprocessing\", os.path.join(ROOT_DIR, \"deep_sort/deep_sort/preprocessing.py\"))\n                gdet = import_from_file(\"generate_detections\", os.path.join(ROOT_DIR, \"deep_sort/tools/generate_detections.py\"))\n                \n                # Create references to classes\n                Detection = detection.Detection\n                Tracker = tracker.Tracker\n                \n                print(\"Successfully imported modules using importlib\")\n            except Exception as e3:\n                print(f\"All import attempts failed: {e3}\")\n                print(\"You may need to restructure your repository or fix the import paths manually\")\n\n# Main execution\nif __name__ == \"__main__\":\n    # Step 1: Download and set up DeepSORT PyTorch\n    download_deepsort_pytorch()\n    \n    # Step 2: Set up TensorFlow model\n    setup_tensorflow_model()\n    \n    # Step 3: Show final directory structure for debugging\n    print(\"\\nFinal directory structure:\")\n    list_directory(os.path.join(ROOT_DIR, 'deep_sort'))\n    \n    # Step 4: Test imports\n    setup_and_test_imports()\n    \n    # Final status\n    tf_model_path = os.path.join(ROOT_DIR, 'deep_sort/resources/networks/mars-small128.pb')\n    pt_model_path = os.path.join(ROOT_DIR, 'deep_sort/deep_sort/deep/checkpoint/ckpt.t7')\n    \n    print(\"\\nFinal setup status:\")\n    print(f\"TensorFlow model: {tf_model_path}\")\n    if os.path.exists(tf_model_path):\n        print(f\"TensorFlow model size: {os.path.getsize(tf_model_path)} bytes\")\n    else:\n        print(\"TensorFlow model not found\")\n        \n    print(f\"PyTorch model: {pt_model_path}\")\n    if os.path.exists(pt_model_path):\n        print(f\"PyTorch model size: {os.path.getsize(pt_model_path)} bytes\")\n    else:\n        print(\"PyTorch model not found\")\n    \n    print(\"\\nSetup process completed\")\n    print(\"You can now use the following import statements in your code:\")\n    print(\"from deep_sort import nn_matching\")\n    print(\"from deep_sort.detection import Detection\")\n    print(\"from deep_sort.tracker import Tracker\")\n    print(\"import tools.generate_detections as gdet\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport cv2\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom ultralytics import YOLO\n\n# Fix import paths\ncurrent_dir = os.getcwd()\nsys.path.append(os.path.dirname(current_dir))\nfrom deep_sort import nn_matching\nfrom deep_sort.detection import Detection\nfrom deep_sort.tracker import Tracker\nimport tools.generate_detections as gdet\n\n\nclass VideoTracker:\n    def __init__(self, video_path, output_path, model_path='yolov8n.pt', \n                 conf_threshold=0.5, iou_threshold=0.3, \n                 max_cosine_distance=0.15, nn_budget=150,\n                 max_age=40, min_hits=2, min_box_area=500):\n        self.video_path = video_path\n        self.output_path = output_path\n        self.conf_threshold = conf_threshold\n        self.iou_threshold = iou_threshold\n        self.min_box_area = min_box_area  # Minimum area to assign an ID\n        \n        # Initialize YOLO model\n        self.model = YOLO(model_path)\n        \n        # Initialize DeepSORT with optimized params for re-ID\n        encoder_model_filename = os.path.join(current_dir, 'deep_sort/resources/networks/mars-small128.pb')\n        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=32)\n        \n        # Improved distance metric for better re-ID\n        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n        \n        # Tracker with parameters tuned for better ID persistence\n        self.tracker = Tracker(metric, max_age=max_age, n_init=min_hits)\n        \n        # Generate more distinctive colors for better ID visualization\n        np.random.seed(42)\n        self.colors = np.random.randint(0, 255, size=(500, 3), dtype=np.uint8)\n        \n        # Track these classes\n        self.class_filter = {\n            0: \"person\",\n            2: \"car\", \n            3: \"motorcycle\",\n            5: \"bus\",\n            7: \"truck\"\n        }\n    \n    def get_filtered_detections(self, frame, results):\n        \"\"\"Extract and filter detections from YOLO results\"\"\"\n        boxes = []\n        scores = []\n        class_ids = []\n        height, width = frame.shape[:2]\n        \n        for r in results:\n            dets = r.boxes.data.cpu().numpy()\n            for det in dets:\n                x1, y1, x2, y2, conf, cls = det\n                cls = int(cls)\n                \n                # Filter by class and confidence\n                if cls in self.class_filter and conf >= self.conf_threshold:\n                    # Ensure box is within frame boundaries\n                    x1 = max(0, int(x1))\n                    y1 = max(0, int(y1))\n                    x2 = min(width, int(x2))\n                    y2 = min(height, int(y2))\n                    \n                    # Calculate box area\n                    w, h = x2-x1, y2-y1\n                    area = w * h\n                    \n                    # Only add boxes with sufficient area\n                    if area >= self.min_box_area:\n                        boxes.append([x1, y1, w, h])\n                        scores.append(float(conf))\n                        class_ids.append(cls)\n        \n        return np.array(boxes), np.array(scores), np.array(class_ids)\n    \n    def run_on_sequence(self, sequence_dir, annotation_file=None):\n        # Get list of all images in the sequence sorted numerically\n        image_files = sorted([f for f in os.listdir(sequence_dir) if f.endswith(('.jpg', '.png'))], \n                            key=lambda x: int(x.split('.')[0]))\n        \n        # Create output video writer\n        first_img_path = os.path.join(sequence_dir, image_files[0])\n        first_img = cv2.imread(first_img_path)\n        if first_img is None:\n            print(f\"Failed to read image: {first_img_path}\")\n            return None\n            \n        height, width = first_img.shape[:2]\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        video_writer = cv2.VideoWriter(self.output_path, fourcc, 30.0, (width, height))\n        \n        # Process each frame\n        frame_id = 0\n        for image_file in tqdm(image_files, desc=\"Processing frames\"):\n            # Read image\n            img_path = os.path.join(sequence_dir, image_file)\n            frame = cv2.imread(img_path)\n            if frame is None:\n                print(f\"Failed to read image: {img_path}\")\n                continue\n            \n            # Make a copy for drawing\n            vis_frame = frame.copy()\n            \n            # Run YOLO detection with improved NMS\n            results = self.model(frame, conf=self.conf_threshold, iou=self.iou_threshold, \n                                classes=list(self.class_filter.keys()))\n            \n            # Extract and filter detections\n            boxes, scores, class_ids = self.get_filtered_detections(frame, results)\n            \n            if len(boxes) > 0:\n                # Extract features with larger batch\n                features = self.encoder(frame, boxes)\n                \n                # Create detection objects\n                detections = []\n                for bbox, score, class_id, feature in zip(boxes, scores, class_ids, features):\n                    detection = Detection(bbox, score, feature)\n                    detection.class_id = class_id  # Add class ID as attribute\n                    detections.append(detection)\n                \n                # Update tracker\n                self.tracker.predict()\n                self.tracker.update(detections)\n                \n                # Draw tracking results\n                for track in self.tracker.tracks:\n                    if not track.is_confirmed() or track.time_since_update > 1:\n                        continue\n                    \n                    # Get bounding box\n                    bbox = track.to_tlwh()\n                    x, y, w, h = bbox\n                    \n                    # Skip tiny boxes\n                    if w*h < self.min_box_area:\n                        continue\n                    \n                    # Get class ID and assign color based on track ID\n                    color_idx = track.track_id % len(self.colors)\n                    color = self.colors[color_idx].tolist()\n                    \n                    # Draw bounding box (thicker for better visibility)\n                    cv2.rectangle(vis_frame, (int(x), int(y)), (int(x+w), int(y+h)), color, 2)\n\n                    # Draw ID with larger text for better visibility\n                    label = f\"ID:{track.track_id}\"\n                    cv2.putText(vis_frame, label, (int(x), int(y-10)), \n                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n            \n            # Write frame to output video\n            video_writer.write(vis_frame)\n            frame_id += 1\n        \n        # Release video writer\n        video_writer.release()\n        print(f\"Tracking completed. Output saved to {self.output_path}\")\n        return self.output_path\n\n\ndef run_tracking_on_sequence(sequence_dir, output_path='output.mp4', \n                            model_path='yolov8n.pt', conf_threshold=0.5, iou_threshold=0.3):\n    \"\"\"Run tracking on a sequence of images with focus on re-ID and handling overlaps\"\"\"\n    \n    # Create tracker with optimized parameters for re-ID\n    tracker = VideoTracker(\n        video_path=sequence_dir,\n        output_path=output_path,\n        model_path=model_path,\n        conf_threshold=conf_threshold,\n        iou_threshold=iou_threshold,\n        max_cosine_distance=0.15,  # Stricter feature matching for better re-ID\n        nn_budget=150,            # Keep more features in memory for re-ID\n        max_age=40,               # Allow tracks to persist longer when occluded\n        min_hits=2,               # Require fewer consecutive detections to confirm tracks\n        min_box_area=500          # Minimum box area to assign an ID (filter distant objects)\n    )\n    \n    # Run tracking\n    return tracker.run_on_sequence(sequence_dir)\n\n\ndef process_visdrone_sequence(base_dir, sequence_name, output_dir='outputs'):\n    \"\"\"Process a specific sequence from VisDrone dataset\"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine paths\n    sequence_dir = os.path.join(base_dir, 'sequences', sequence_name)\n    output_path = os.path.join(output_dir, f\"{sequence_name}_tracked.mp4\")\n    \n    # Check if files exist\n    if not os.path.exists(sequence_dir):\n        print(f\"Sequence directory not found: {sequence_dir}\")\n        return None\n    \n    # Run tracking with optimized parameters\n    return run_tracking_on_sequence(\n        sequence_dir=sequence_dir,\n        output_path=output_path,\n        conf_threshold=0.5,  # High confidence for reliable detections\n        iou_threshold=0.3    # Lower IoU for better handling of overlaps\n    )\n\n\nif __name__ == \"__main__\":\n    # Set base directory for your dataset\n    base_dir = \"/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-test-dev/VisDrone2019-MOT-test-dev\"\n    \n    # Choose a sequence to process\n    sequence_name = \"uav0000120_04775_v\"\n    \n    # Create an output directory in the Kaggle working directory\n    output_dir = \"/kaggle/working/tracking_outputs\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Run the tracking with focus on re-ID and handling overlaps\n    results = process_visdrone_sequence(base_dir, sequence_name, output_dir)\n    \n    if results:\n        print(f\"Tracking completed successfully: {results}\")\n    else:\n        print(\"Tracking failed. Check the error messages above.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nfrom scipy.optimize import linear_sum_assignment\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\nclass MOTEvaluator:\n    def __init__(self, gt_file, result_file, iou_threshold=0.5):\n        \"\"\"\n        Initialize MOT evaluator\n        gt_file: path to ground truth file in MOT format\n        result_file: path to result file in MOT format\n        iou_threshold: IoU threshold for matching detections\n        \"\"\"\n        self.gt_file = gt_file\n        self.result_file = result_file\n        self.iou_threshold = iou_threshold\n        \n        # Load ground truth and results\n        self.gt_data = self.load_mot_file(gt_file)\n        self.res_data = self.load_mot_file(result_file)\n        \n        # Results\n        self.metrics = {\n            'MOTA': 0,      # Multiple Object Tracking Accuracy\n            'MOTP': 0,      # Multiple Object Tracking Precision\n            'IDF1': 0,      # ID F1 Score\n            'IDSw': 0,      # ID Switches\n            'MT': 0,        # Mostly Tracked\n            'ML': 0,        # Mostly Lost\n            'FP': 0,        # False Positives\n            'FN': 0,        # False Negatives\n            'Recall': 0,    # Recall\n            'Precision': 0  # Precision\n        }\n\n    def load_mot_file(self, file_path):\n        \"\"\"Load MOT format file into dictionary by frame\"\"\"\n        if not os.path.exists(file_path):\n            print(f\"File not found: {file_path}\")\n            return {}\n        \n        data = defaultdict(list)\n        try:\n            # MOT format: frame_id, track_id, x, y, w, h, conf, x, y, z (last 3 optional)\n            df = pd.read_csv(file_path, header=None)\n            for _, row in df.iterrows():\n                frame_id = int(row[0])\n                track_id = int(row[1])\n                bbox = [float(row[2]), float(row[3]), float(row[4]), float(row[5])]\n                conf = float(row[6]) if len(row) > 6 else 1.0\n                \n                data[frame_id].append({\n                    'track_id': track_id,\n                    'bbox': bbox,\n                    'conf': conf\n                })\n        except Exception as e:\n            print(f\"Error loading MOT file: {e}\")\n        \n        return data\n\n    def convert_to_mot_format(self, tracks_by_frame):\n        \"\"\"Convert tracking results to MOT format\"\"\"\n        mot_data = []\n        \n        for frame_id, tracks in tracks_by_frame.items():\n            for track in tracks:\n                # frame_id, track_id, x, y, w, h, conf, -1, -1, -1\n                x, y, w, h = track['bbox']\n                mot_data.append([\n                    frame_id, track['track_id'], x, y, w, h, \n                    track['conf'], -1, -1, -1\n                ])\n        \n        return pd.DataFrame(mot_data)\n\n    def save_mot_results(self, file_path):\n        \"\"\"Save results in MOT format\"\"\"\n        if not self.res_data:\n            print(\"No results to save\")\n            return\n            \n        mot_df = self.convert_to_mot_format(self.res_data)\n        mot_df.to_csv(file_path, header=False, index=False)\n        print(f\"Results saved to {file_path}\")\n\n    @staticmethod\n    def calculate_iou(bbox1, bbox2):\n        \"\"\"Calculate IoU between two bounding boxes [x,y,w,h]\"\"\"\n        x1, y1, w1, h1 = bbox1\n        x2, y2, w2, h2 = bbox2\n        \n        # Calculate intersection area\n        x_left = max(x1, x2)\n        y_top = max(y1, y2)\n        x_right = min(x1 + w1, x2 + w2)\n        y_bottom = min(y1 + h1, y2 + h2)\n        \n        if x_right < x_left or y_bottom < y_top:\n            return 0.0\n            \n        intersection_area = (x_right - x_left) * (y_bottom - y_top)\n        \n        # Calculate union area\n        bbox1_area = w1 * h1\n        bbox2_area = w2 * h2\n        union_area = bbox1_area + bbox2_area - intersection_area\n        \n        # Calculate IoU\n        iou = intersection_area / union_area if union_area > 0 else 0\n        return iou\n\n    def match_detections(self, gt_detections, res_detections):\n        \"\"\"Match ground truth detections with result detections using IoU\"\"\"\n        if not gt_detections or not res_detections:\n            return [], [], []\n            \n        # Calculate IoU matrix\n        cost_matrix = np.zeros((len(gt_detections), len(res_detections)))\n        for i, gt in enumerate(gt_detections):\n            for j, res in enumerate(res_detections):\n                iou = self.calculate_iou(gt['bbox'], res['bbox'])\n                # Negative because linear_sum_assignment minimizes cost\n                cost_matrix[i, j] = -iou\n        \n        # Find optimal assignment\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        # Filter matches by IoU threshold\n        matches = []\n        unmatched_gt = list(range(len(gt_detections)))\n        unmatched_res = list(range(len(res_detections)))\n        \n        for r, c in zip(row_ind, col_ind):\n            if -cost_matrix[r, c] >= self.iou_threshold:\n                matches.append((r, c))\n                if r in unmatched_gt:\n                    unmatched_gt.remove(r)\n                if c in unmatched_res:\n                    unmatched_res.remove(c)\n        \n        return matches, unmatched_gt, unmatched_res\n\n    def evaluate(self):\n        \"\"\"Evaluate tracking results\"\"\"\n        total_gt = 0\n        total_res = 0\n        total_matches = 0\n        total_iou = 0\n        id_switches = 0\n        \n        false_positives = 0\n        false_negatives = 0\n        \n        # Track ID assignments for ID switch calculation\n        last_matched_res_id = {}\n        \n        # Track statistics for MT/ML calculation\n        gt_track_frames = defaultdict(int)\n        gt_track_matches = defaultdict(int)\n        \n        # Get all unique frame IDs\n        all_frames = sorted(set(list(self.gt_data.keys()) + list(self.res_data.keys())))\n        \n        # Count total ground truth detections per track\n        for frame_id, detections in self.gt_data.items():\n            for det in detections:\n                gt_track_frames[det['track_id']] += 1\n        \n        for frame_id in tqdm(all_frames, desc=\"Evaluating\"):\n            gt_detections = self.gt_data.get(frame_id, [])\n            res_detections = self.res_data.get(frame_id, [])\n            \n            total_gt += len(gt_detections)\n            total_res += len(res_detections)\n            \n            # Match detections\n            matches, unmatched_gt, unmatched_res = self.match_detections(gt_detections, res_detections)\n            \n            total_matches += len(matches)\n            false_positives += len(unmatched_res)\n            false_negatives += len(unmatched_gt)\n            \n            # Calculate IoU for matched detections\n            for gt_idx, res_idx in matches:\n                gt_det = gt_detections[gt_idx]\n                res_det = res_detections[res_idx]\n                \n                iou = self.calculate_iou(gt_det['bbox'], res_det['bbox'])\n                total_iou += iou\n                \n                # Update track matches count\n                gt_track_matches[gt_det['track_id']] += 1\n                \n                # Check for ID switches\n                if gt_det['track_id'] in last_matched_res_id:\n                    if last_matched_res_id[gt_det['track_id']] != res_det['track_id']:\n                        id_switches += 1\n                \n                # Update last matched result ID\n                last_matched_res_id[gt_det['track_id']] = res_det['track_id']\n        \n        # Calculate metrics\n        if total_matches > 0:\n            self.metrics['MOTP'] = total_iou / total_matches\n        \n        if total_gt > 0:\n            self.metrics['MOTA'] = 1 - (false_positives + false_negatives + id_switches) / total_gt\n            self.metrics['Recall'] = total_matches / total_gt\n        \n        if total_res > 0:\n            self.metrics['Precision'] = total_matches / total_res\n        \n        if self.metrics['Precision'] + self.metrics['Recall'] > 0:\n            self.metrics['IDF1'] = 2 * self.metrics['Precision'] * self.metrics['Recall'] / (self.metrics['Precision'] + self.metrics['Recall'])\n        \n        self.metrics['IDSw'] = id_switches\n        self.metrics['FP'] = false_positives\n        self.metrics['FN'] = false_negatives\n        \n        # Calculate MT/ML metrics (Mostly Tracked / Mostly Lost)\n        mostly_tracked = 0\n        mostly_lost = 0\n        \n        for track_id, total_frames in gt_track_frames.items():\n            match_ratio = gt_track_matches[track_id] / total_frames\n            if match_ratio >= 0.8:\n                mostly_tracked += 1\n            elif match_ratio <= 0.2:\n                mostly_lost += 1\n        \n        self.metrics['MT'] = mostly_tracked\n        self.metrics['ML'] = mostly_lost\n        \n        return self.metrics\n\n    def print_metrics(self):\n        \"\"\"Print evaluation metrics\"\"\"\n        print(\"\\nTracking Evaluation Results:\")\n        print(\"----------------------------\")\n        for metric, value in self.metrics.items():\n            print(f\"{metric}: {value:.4f}\")\n\n    def plot_metrics(self, output_file=None):\n        \"\"\"Plot evaluation metrics\"\"\"\n        metrics = list(self.metrics.keys())\n        values = list(self.metrics.values())\n        \n        plt.figure(figsize=(10, 6))\n        plt.bar(metrics, values)\n        plt.ylabel('Score')\n        plt.title('Tracking Evaluation Metrics')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        \n        if output_file:\n            plt.savefig(output_file)\n        else:\n            plt.show()\n\n\ndef convert_results_to_mot_format(tracking_results, output_file):\n    \"\"\"\n    Convert tracking results to MOT format\n    tracking_results: dictionary with keys as frame_id and values as list of dictionaries\n                     with track_id, bbox (x,y,w,h), and confidence\n    output_file: path to output MOT format file\n    \"\"\"\n    with open(output_file, 'w') as f:\n        for frame_id, tracks in sorted(tracking_results.items()):\n            for track in tracks:\n                # MOT format: frame_id, track_id, x, y, w, h, conf, -1, -1, -1\n                x, y, w, h = track['bbox']\n                line = f\"{frame_id},{track['track_id']},{x},{y},{w},{h},{track['conf']},-1,-1,-1\\n\"\n                f.write(line)\n    \n    print(f\"Results saved to {output_file}\")\n\n\n# Add this function to extract tracking results from a video\ndef extract_tracking_results_from_video(video_path, output_mot_file):\n    \"\"\"\n    Process a video file with tracking and save results in MOT format\n    \"\"\"\n    # Initialize YOLO model and tracker\n    tracker = VideoTracker(\n        video_path=video_path,\n        output_path=\"temp_output.mp4\",  # Temporary output path\n        conf_threshold=0.5,\n        iou_threshold=0.3\n    )\n    \n    # Open video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error opening video file: {video_path}\")\n        return None\n    \n    # Process each frame\n    frame_id = 0\n    tracking_results = defaultdict(list)\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n            \n        # Run YOLO detection\n        results = tracker.model(frame, conf=tracker.conf_threshold, iou=tracker.iou_threshold, \n                            classes=list(tracker.class_filter.keys()))\n        \n        # Extract and filter detections\n        boxes, scores, class_ids = tracker.get_filtered_detections(frame, results)\n        \n        if len(boxes) > 0:\n            # Extract features\n            features = tracker.encoder(frame, boxes)\n            \n            # Create detection objects\n            detections = []\n            for bbox, score, class_id, feature in zip(boxes, scores, class_ids, features):\n                detection = Detection(bbox, score, feature)\n                detection.class_id = class_id\n                detections.append(detection)\n            \n            # Update tracker\n            tracker.tracker.predict()\n            tracker.tracker.update(detections)\n            \n            # Store tracking results\n            for track in tracker.tracker.tracks:\n                if not track.is_confirmed() or track.time_since_update > 1:\n                    continue\n                \n                bbox = track.to_tlwh()\n                x, y, w, h = bbox\n                \n                if w*h < tracker.min_box_area:\n                    continue\n                    \n                tracking_results[frame_id].append({\n                    'track_id': track.track_id,\n                    'bbox': [x, y, w, h],\n                    'conf': 1.0\n                })\n        \n        frame_id += 1\n    \n    # Release resources\n    cap.release()\n    \n    # Convert and save results\n    convert_results_to_mot_format(tracking_results, output_mot_file)\n    \n    return output_mot_file\n\n# Example usage:\nif __name__ == \"__main__\":\n    # If you only have the video output and want to extract tracking results\n    video_path = \"/kaggle/working/tracking_outputs/uav0000120_04775_v_tracked.mp4\"\n    output_mot_file = \"/kaggle/working/tracking_outputs/uav0000120_04775_v_mot.txt\"\n    \n    result_file = extract_tracking_results_from_video(video_path, output_mot_file)\n    print(f\"Tracking results saved to: {result_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick evaluation directly in notebook\ngt_file = \"/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-test-dev/VisDrone2019-MOT-test-dev/annotations/uav0000120_04775_v.txt\"\nresult_file = \"/kaggle/working/tracking_outputs/uav0000120_04775_v_mot.txt\"\n\nevaluator = MOTEvaluator(gt_file, result_file, iou_threshold=0.5)\nmetrics = evaluator.evaluate()\nevaluator.print_metrics()\nevaluator.plot_metrics()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:58:10.029796Z","iopub.execute_input":"2025-04-17T05:58:10.030160Z","iopub.status.idle":"2025-04-17T05:58:13.170566Z","shell.execute_reply.started":"2025-04-17T05:58:10.030129Z","shell.execute_reply":"2025-04-17T05:58:13.169662Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 4634.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTracking Evaluation Results:\n----------------------------\nMOTA: 0.1068\nMOTP: 0.7942\nIDF1: 0.1290\nIDSw: 86.0000\nMT: 2.0000\nML: 336.0000\nFP: 266.0000\nFN: 40629.0000\nRecall: 0.0693\nPrecision: 0.9228\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":61}]}