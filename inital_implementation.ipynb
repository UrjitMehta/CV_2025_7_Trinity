{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10804236,"sourceType":"datasetVersion","datasetId":6706070}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/tracking_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T11:15:49.220549Z","iopub.execute_input":"2025-03-19T11:15:49.220900Z","iopub.status.idle":"2025-03-19T11:15:49.418872Z","shell.execute_reply.started":"2025-03-19T11:15:49.220875Z","shell.execute_reply":"2025-03-19T11:15:49.417696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install ultralytics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T09:59:43.073741Z","iopub.execute_input":"2025-03-19T09:59:43.074167Z","iopub.status.idle":"2025-03-19T09:59:47.625826Z","shell.execute_reply.started":"2025-03-19T09:59:43.074143Z","shell.execute_reply":"2025-03-19T09:59:47.624829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare the dataset for YOLOv8","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport yaml\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\n\n# Define the paths\nDATASET_PATH = \"/kaggle/input/cv-multiobject-dectection-dataset\"\nVISDRONE_MOT_TRAIN = os.path.join(DATASET_PATH, \"VisDrone2019-MOT-train/VisDrone2019-MOT-train\")\nVISDRONE_MOT_VAL = os.path.join(DATASET_PATH, \"VisDrone2019-MOT-val/VisDrone2019-MOT-val\")\nVISDRONE_MOT_TEST = os.path.join(DATASET_PATH, \"VisDrone2019-MOT-test-challenge/VisDrone2019-MOT-test-challenge\")\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\n\n# Create output directories\nos.makedirs(OUTPUT_PATH, exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"images\", \"train\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"images\", \"val\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"images\", \"test\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"labels\", \"train\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"labels\", \"val\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_PATH, \"labels\", \"test\"), exist_ok=True)\n\n# VisDrone MOT categories\n# The VisDrone annotation format appears to be:\n# <frame_index>,<target_id>,<bbox_left>,<bbox_top>,<bbox_width>,<bbox_height>,<score>,<object_category>,<truncation>,<occlusion>\n#\n# The object_category is defined as:\n# 0: ignored regions\n# 1: pedestrian\n# 2: people\n# 3: bicycle\n# 4: car\n# 5: van\n# 6: truck\n# 7: tricycle\n# 8: awning-tricycle\n# 9: bus\n# 10: motor\n# 11: others\n\n# Define the class mapping - we'll map VisDrone classes to YOLOv8 format\n# We'll skip class 0 (ignored regions) and class 11 (others)\nclass_mapping = {\n    1: 0,  # pedestrian\n    2: 1,  # people\n    3: 2,  # bicycle\n    4: 3,  # car\n    5: 4,  # van\n    6: 5,  # truck\n    7: 6,  # tricycle\n    8: 7,  # awning-tricycle\n    9: 8,  # bus\n    10: 9,  # motor\n}\n\n# Class names for YAML file\nclass_names = [\n    'pedestrian',\n    'people',\n    'bicycle',\n    'car',\n    'van',\n    'truck',\n    'tricycle',\n    'awning-tricycle',\n    'bus',\n    'motor'\n]\n\ndef convert_annotation(annotation_file, sequence_folder, split_type):\n    \"\"\"\n    Convert VisDrone annotation format to YOLO format\n    \"\"\"\n    sequence_name = os.path.basename(sequence_folder)\n    \n    # Read the annotation file\n    with open(annotation_file, 'r') as f:\n        lines = f.readlines()\n    \n    # Group annotations by frame\n    frame_annotations = {}\n    for line in lines:\n        line = line.strip().split(',')\n        frame_idx = int(line[0])\n        if frame_idx not in frame_annotations:\n            frame_annotations[frame_idx] = []\n        frame_annotations[frame_idx].append(line)\n    \n    # Process each frame\n    for frame_idx, annotations in frame_annotations.items():\n        # Source image path\n        img_file = f\"{frame_idx:07d}.jpg\"\n        img_path = os.path.join(sequence_folder, img_file)\n        \n        if not os.path.exists(img_path):\n            continue\n        \n        # Destination paths\n        dst_img_path = os.path.join(OUTPUT_PATH, \"images\", split_type, f\"{sequence_name}_{img_file}\")\n        dst_label_path = os.path.join(OUTPUT_PATH, \"labels\", split_type, f\"{sequence_name}_{img_file.replace('.jpg', '.txt')}\")\n        \n        # Copy the image\n        shutil.copy(img_path, dst_img_path)\n        \n        # Convert annotations to YOLO format and write to file\n        with open(dst_label_path, 'w') as f:\n            # Get image dimensions\n            # We'll use PIL to get the image dimensions\n            from PIL import Image\n            img = Image.open(img_path)\n            img_width, img_height = img.size\n            \n            for anno in annotations:\n                obj_id = int(anno[1])\n                bbox_left = float(anno[2])\n                bbox_top = float(anno[3])\n                bbox_width = float(anno[4])\n                bbox_height = float(anno[5])\n                obj_class = int(anno[7])\n                \n                # Skip ignored regions and others\n                if obj_class == 0 or obj_class == 11:\n                    continue\n                \n                # Map to YOLO class\n                if obj_class not in class_mapping:\n                    continue\n                yolo_class = class_mapping[obj_class]\n                \n                # Convert bbox to YOLO format (x_center, y_center, width, height) - normalized\n                x_center = (bbox_left + bbox_width / 2) / img_width\n                y_center = (bbox_top + bbox_height / 2) / img_height\n                width = bbox_width / img_width\n                height = bbox_height / img_height\n                \n                # Write to file\n                f.write(f\"{yolo_class} {x_center} {y_center} {width} {height}\\n\")\n\ndef process_dataset():\n    \"\"\"\n    Process the dataset using the existing train/val/test splits\n    \"\"\"\n    # Process training data\n    train_sequences_folder = os.path.join(VISDRONE_MOT_TRAIN, \"sequences\")\n    train_annotations_folder = os.path.join(VISDRONE_MOT_TRAIN, \"annotations\")\n    \n    train_sequences = [f for f in os.listdir(train_sequences_folder) if os.path.isdir(os.path.join(train_sequences_folder, f))]\n    \n    for seq in tqdm(train_sequences, desc=\"Processing train set\"):\n        seq_path = os.path.join(train_sequences_folder, seq)\n        anno_path = os.path.join(train_annotations_folder, f\"{seq}.txt\")\n        if os.path.exists(anno_path):\n            convert_annotation(anno_path, seq_path, \"train\")\n    \n    # Process validation data\n    val_sequences_folder = os.path.join(VISDRONE_MOT_VAL, \"sequences\")\n    val_annotations_folder = os.path.join(VISDRONE_MOT_VAL, \"annotations\")\n    \n    val_sequences = [f for f in os.listdir(val_sequences_folder) if os.path.isdir(os.path.join(val_sequences_folder, f))]\n    \n    for seq in tqdm(val_sequences, desc=\"Processing validation set\"):\n        seq_path = os.path.join(val_sequences_folder, seq)\n        anno_path = os.path.join(val_annotations_folder, f\"{seq}.txt\")\n        if os.path.exists(anno_path):\n            convert_annotation(anno_path, seq_path, \"val\")\n    \n    # Process test data\n    test_sequences_folder = os.path.join(VISDRONE_MOT_TEST, \"sequences\")\n    test_annotations_folder = os.path.join(VISDRONE_MOT_TEST, \"annotations\")\n    \n    test_sequences = [f for f in os.listdir(test_sequences_folder) if os.path.isdir(os.path.join(test_sequences_folder, f))]\n    \n    for seq in tqdm(test_sequences, desc=\"Processing test set\"):\n        seq_path = os.path.join(test_sequences_folder, seq)\n        # For test-challenge, we might not have annotations\n        anno_path = os.path.join(test_annotations_folder, f\"{seq}.txt\")\n        if os.path.exists(anno_path):\n            convert_annotation(anno_path, seq_path, \"test\")\n        else:\n            # If no annotations, just copy images\n            for img_file in os.listdir(seq_path):\n                if img_file.endswith('.jpg'):\n                    src_img_path = os.path.join(seq_path, img_file)\n                    dst_img_path = os.path.join(OUTPUT_PATH, \"images\", \"test\", f\"{seq}_{img_file}\")\n                    shutil.copy(src_img_path, dst_img_path)\n\ndef create_yaml_file():\n    \"\"\"\n    Create YAML configuration file for YOLOv8\n    \"\"\"\n    yaml_content = {\n        'path': OUTPUT_PATH,\n        'train': os.path.join('images', 'train'),\n        'val': os.path.join('images', 'val'),\n        'test': os.path.join('images', 'test'),\n        'nc': len(class_names),  # number of classes\n        'names': class_names\n    }\n    \n    with open(os.path.join(OUTPUT_PATH, 'visdrone.yaml'), 'w') as f:\n        yaml.dump(yaml_content, f, default_flow_style=False)\n\nif __name__ == \"__main__\":\n    print(\"Starting dataset preparation...\")\n    process_dataset()\n    create_yaml_file()\n    \n    # Print some statistics\n    train_images = len(os.listdir(os.path.join(OUTPUT_PATH, \"images\", \"train\")))\n    val_images = len(os.listdir(os.path.join(OUTPUT_PATH, \"images\", \"val\")))\n    test_images = len(os.listdir(os.path.join(OUTPUT_PATH, \"images\", \"test\")))\n    \n    print(f\"Dataset preparation complete!\")\n    print(f\"Train images: {train_images}\")\n    print(f\"Validation images: {val_images}\")\n    print(f\"Test images: {test_images}\")\n    print(f\"YAML configuration file created at: {os.path.join(OUTPUT_PATH, 'visdrone.yaml')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:07:32.863674Z","iopub.execute_input":"2025-03-19T10:07:32.864034Z","iopub.status.idle":"2025-03-19T10:15:38.013888Z","shell.execute_reply.started":"2025-03-19T10:07:32.864009Z","shell.execute_reply":"2025-03-19T10:15:38.013038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure YOLOv8 model","metadata":{}},{"cell_type":"code","source":"import yaml\nfrom ultralytics import YOLO\nimport os\n\n# Set the paths\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\nMODELS_PATH = \"/kaggle/working/models\"\nYAML_PATH = os.path.join(OUTPUT_PATH, \"visdrone.yaml\")\n\n# Create models directory if it doesn't exist\nos.makedirs(MODELS_PATH, exist_ok=True)\n\ndef select_model_size():\n    \"\"\"\n    Select the appropriate model size based on your requirements\n    - nano: Fastest but least accurate (ideal for resource-constrained environments)\n    - small: Good balance between speed and accuracy\n    - medium: More accurate but slower than small\n    - large: Most accurate but slowest\n    \"\"\"\n    model_sizes = {\n        'n': 'yolov8n',  # Nano\n        's': 'yolov8s',  # Small\n        'm': 'yolov8m',  # Medium\n        'l': 'yolov8l',  # Large\n        'x': 'yolov8x'   # Extra Large\n    }\n    \n    print(\"Select YOLOv8 model size:\")\n    print(\"n - Nano (fastest, least accurate)\")\n    print(\"s - Small (good balance)\")\n    print(\"m - Medium (more accurate, slower)\")\n    print(\"l - Large (most accurate, slowest)\")\n    print(\"x - Extra Large (highest accuracy, very slow)\")\n    \n    choice = input(\"Enter your choice (default: s): \").lower() or 's'\n    if choice not in model_sizes:\n        print(f\"Invalid choice: {choice}. Using Small model.\")\n        choice = 's'\n    \n    model_name = model_sizes[choice]\n    print(f\"Selected model: {model_name}\")\n    return model_name\n\ndef load_config():\n    \"\"\"Load the dataset configuration from YAML file\"\"\"\n    with open(YAML_PATH, 'r') as f:\n        config = yaml.safe_load(f)\n    return config\n\ndef configure_model(model_size):\n    \"\"\"Configure the YOLOv8 model with the selected size\"\"\"\n\n    model_path = os.path.join(MODELS_PATH, f\"{model_size}.pt\")\n    \n    # Check if the model file exists, otherwise download it\n    if not os.path.exists(model_path):\n        print(f\"ðŸ”½ {model_size}.pt not found. Downloading it now...\")\n        model = YOLO(model_size)  # This will automatically download the model\n        model.export(format=\"torch\")  # Save it in the correct format\n        os.rename(f\"{model_size}.pt\", model_path)  # Move to desired location\n        print(f\"âœ… Model downloaded and saved at {model_path}\")\n    else:\n        print(f\"âœ… Model found at {model_path}\")\n\n    # Load the pre-trained model\n    model = YOLO(model_path)\n    \n    # Model configuration\n    model_config = {\n        'task': 'detect',\n        'model': model_size,\n        'data': YAML_PATH,\n        'epochs': 100,\n        'patience': 10,\n        'batch': 16,\n        'imgsz': 640,\n        'device': 0,  # Use GPU if available\n        'workers': 8,\n        'project': MODELS_PATH,\n        'name': f'visdrone_{model_size.lower()}',\n    }\n    \n    # Save the configuration\n    config_path = os.path.join(MODELS_PATH, f\"{model_size.lower()}_config.yaml\")\n    with open(config_path, 'w') as f:\n        yaml.dump(model_config, f)\n    \n    print(f\"ðŸ“„ Model configuration saved to {config_path}\")\n    return model_config\n\ndef main():\n    \"\"\"Main function to configure the YOLOv8 model\"\"\"\n    dataset_config = load_config()\n    print(f\"Dataset configuration loaded from {YAML_PATH}\")\n    print(f\"Number of classes: {dataset_config['nc']}\")\n    print(f\"Classes: {dataset_config['names']}\")\n    \n    model_size = select_model_size()\n    model_config = configure_model(model_size)\n    \n    print(\"\\nModel configuration complete!\")\n    print(\"Next steps:\")\n    print(\"Run the training script\")\n    print(\"Monitor training metrics\")\n    print(\"Evaluate model performance\")\n    \n    return model_config\n\nif __name__ == \"__main__\":\n    model_config = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:16:42.072348Z","iopub.execute_input":"2025-03-19T10:16:42.072708Z","iopub.status.idle":"2025-03-19T10:16:54.913737Z","shell.execute_reply.started":"2025-03-19T10:16:42.072683Z","shell.execute_reply":"2025-03-19T10:16:54.912758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GPU","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:21.069649Z","iopub.execute_input":"2025-03-19T10:17:21.070098Z","iopub.status.idle":"2025-03-19T10:17:24.556642Z","shell.execute_reply.started":"2025-03-19T10:17:21.070073Z","shell.execute_reply":"2025-03-19T10:17:24.555589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True\nprint(torch.cuda.get_device_name(0))  # Should say Tesla T4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:26.389267Z","iopub.execute_input":"2025-03-19T10:17:26.389686Z","iopub.status.idle":"2025-03-19T10:17:26.395204Z","shell.execute_reply.started":"2025-03-19T10:17:26.389654Z","shell.execute_reply":"2025-03-19T10:17:26.394503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ultralytics\nprint(f\"Ultralytics YOLOv8 version: {ultralytics.__version__}\")\n\nfrom ultralytics.utils.torch_utils import select_device\ndevice = select_device('')\nprint(f\"YOLOv8 Selected Device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:27.802630Z","iopub.execute_input":"2025-03-19T10:17:27.802932Z","iopub.status.idle":"2025-03-19T10:17:27.808938Z","shell.execute_reply.started":"2025-03-19T10:17:27.802909Z","shell.execute_reply":"2025-03-19T10:17:27.808245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport ultralytics\nfrom ultralytics.utils.torch_utils import select_device\n\nprint(f\"Ultralytics YOLOv8 version: {ultralytics.__version__}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nprint(f\"YOLOv8 Selected Device: {select_device('')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:30.309167Z","iopub.execute_input":"2025-03-19T10:17:30.309516Z","iopub.status.idle":"2025-03-19T10:17:30.316301Z","shell.execute_reply.started":"2025-03-19T10:17:30.309483Z","shell.execute_reply":"2025-03-19T10:17:30.315295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"# Train YOLOv8 Model\nimport os\nimport yaml\nimport torch\nfrom ultralytics import YOLO\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set the paths\nMODELS_PATH = \"/kaggle/working/models\"\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\nYAML_PATH = os.path.join(OUTPUT_PATH, \"visdrone.yaml\")\n\ndef load_model_config(config_path=None):\n    \"\"\"Load model configuration from file or use default\"\"\"\n    if config_path and os.path.exists(config_path):\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n        return config\n    \n    # Default configuration if no file is provided\n    config_files = [f for f in os.listdir(MODELS_PATH) if f.endswith('_config.yaml')]\n    if config_files:\n        config_path = os.path.join(MODELS_PATH, config_files[0])\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n        return config\n    \n    # Fallback to very basic configuration\n    return {\n        'task': 'detect',\n        'model': 'YOLOv8s.pt',\n        'data': YAML_PATH,\n        'epochs': 3,\n        'patience': 10,\n        'batch': 16,\n        'imgsz': 640,\n        'device': 0,\n        'workers': 8,\n        'project': MODELS_PATH,\n        'name': 'visdrone_default',\n    }\n\ndef check_gpu_availability():\n    \"\"\"Check if GPU is available and print info\"\"\"\n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        print(f\"GPU available! Found {device_count} GPU(s).\")\n        for i in range(device_count):\n            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"Current GPU: {torch.cuda.current_device()}\")\n        print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n        print(f\"Memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n    else:\n        print(\"No GPU available. Training will be slow on CPU.\")\n\ndef custom_hyperparameters():\n    \"\"\"Customize hyperparameters for training\"\"\"\n    hyperparams = {}\n    \n    # Learning rate\n    hyperparams['lr0'] = 0.01  # Initial learning rate\n    hyperparams['lrf'] = 0.01  # Final learning rate ratio\n    \n    # Optimizer parameters\n    hyperparams['momentum'] = 0.937\n    hyperparams['weight_decay'] = 0.0005\n    \n    # Augmentation parameters\n    hyperparams['hsv_h'] = 0.015  # HSV-Hue augmentation\n    hyperparams['hsv_s'] = 0.7    # HSV-Saturation augmentation\n    hyperparams['hsv_v'] = 0.4    # HSV-Value augmentation\n    hyperparams['degrees'] = 0.0   # Rotation augmentation\n    hyperparams['translate'] = 0.1  # Translation augmentation\n    hyperparams['scale'] = 0.5     # Scale augmentation\n    hyperparams['fliplr'] = 0.5    # Horizontal flip augmentation\n    hyperparams['mosaic'] = 1.0    # Mosaic augmentation\n    \n    return hyperparams\n\ndef train_model(config, hyperparams=None):\n    \"\"\"Train the YOLOv8 model with the given configuration\"\"\"\n\n    # Ensure we train for only 3 epochs\n    config['epochs'] = 3  \n\n    # Explicitly set device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n    # Load model\n    model = YOLO(config['model'])\n\n    # Train the model\n    results = model.train(\n        data=config['data'],\n        epochs=config['epochs'],\n        patience=config['patience'],\n        batch=config['batch'],\n        imgsz=config['imgsz'],\n        device=device,  # Ensure GPU is used\n        workers=config['workers'],\n        project=config['project'],\n        name=config['name'],\n        exist_ok=True,\n        pretrained=True,\n        **(hyperparams or {})\n    )\n    \n    return model, results\n\n\ndef plot_training_results(results_file):\n    \"\"\"Plot training metrics from results CSV\"\"\"\n    if not os.path.exists(results_file):\n        print(f\"Results file not found: {results_file}\")\n        return\n    \n    # Load results\n    results = pd.read_csv(results_file)\n    \n    # Plot metrics\n    metrics = ['box_loss', 'cls_loss', 'dfl_loss', 'precision', 'recall', 'mAP50', 'mAP50-95']\n    \n    plt.figure(figsize=(20, 14))\n    \n    for i, metric in enumerate(metrics):\n        if metric in results.columns:\n            plt.subplot(3, 3, i+1)\n            plt.plot(results['epoch'], results[metric], 'b-')\n            plt.title(f'Training {metric}')\n            plt.xlabel('Epoch')\n            plt.ylabel(metric)\n            plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(MODELS_PATH, 'training_metrics.png'))\n    plt.show()\n\ndef main():\n    \"\"\"Main function to train the YOLOv8 model\"\"\"\n    # Check GPU availability\n    check_gpu_availability()\n    \n    # Load model configuration\n    config_files = [f for f in os.listdir(MODELS_PATH) if f.endswith('_config.yaml')]\n    if config_files:\n        config_path = os.path.join(MODELS_PATH, config_files[0])\n        print(f\"Loading configuration from {config_path}\")\n        config = load_model_config(config_path)\n    else:\n        print(\"No configuration file found. Using default configuration.\")\n        config = load_model_config()\n    \n    # Customize hyperparameters\n    hyperparams = custom_hyperparameters()\n    \n    # Train the model\n    model, results = train_model(config, hyperparams)\n    \n    # Plot training results\n    results_file = os.path.join(MODELS_PATH, config['name'], 'results.csv')\n    plot_training_results(results_file)\n    \n    print(\"\\nTraining complete!\")\n    print(f\"Model saved at: {os.path.join(MODELS_PATH, config['name'])}\")\n    print(\"Next steps:\")\n    print(\"1. Evaluate model performance\")\n    print(\"2. Run inference on test images\")\n    print(\"3. Export model for deployment\")\n    \n    return model\n\nif __name__ == \"__main__\":\n    model = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:18:33.356787Z","iopub.execute_input":"2025-03-19T10:18:33.357107Z","iopub.status.idle":"2025-03-19T10:47:12.911903Z","shell.execute_reply.started":"2025-03-19T10:18:33.357084Z","shell.execute_reply":"2025-03-19T10:47:12.910011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Detection","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Load your trained model\nmodel_path = \"/kaggle/working/models/visdrone_yolov8s/weights/best.pt\"\nmodel = YOLO(model_path)\n\n# Path to the test image\nimage_path = \"/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-train/VisDrone2019-MOT-train/sequences/uav0000013_00000_v/0000001.jpg\"\n\n# Run inference on the image\nresults = model.predict(image_path, conf=0.25)\n\n# Plot the results\nplt.figure(figsize=(12, 10))\nimg = cv2.imread(image_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nresult_img = results[0].plot()\nplt.imshow(result_img)\nplt.axis('off')\nplt.title(\"Prediction Results\")\nplt.show()\n\n# Print detection statistics\nprint(f\"Number of detections: {len(results[0].boxes)}\")\nprint(f\"Detected classes: {results[0].names}\")\n\n# Print detailed results for top 5 detections\nfor i, box in enumerate(results[0].boxes[:5]):\n    class_id = int(box.cls)\n    confidence = float(box.conf)\n    x1, y1, x2, y2 = box.xyxy[0].tolist()\n    print(f\"Detection {i+1}: Class: {results[0].names[class_id]}, Confidence: {confidence:.2f}, Coordinates: [{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:47:12.914444Z","iopub.execute_input":"2025-03-19T10:47:12.914878Z","iopub.status.idle":"2025-03-19T10:47:13.861774Z","shell.execute_reply.started":"2025-03-19T10:47:12.914837Z","shell.execute_reply":"2025-03-19T10:47:13.861408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate the model","metadata":{}},{"cell_type":"code","source":"# Evaluate YOLOv8 Model\nimport os\nimport yaml\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport torch\nimport cv2\n\n# Set the paths\nMODELS_PATH = \"/kaggle/working/models\"\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\nYAML_PATH = os.path.join(OUTPUT_PATH, \"visdrone.yaml\")\nRESULTS_PATH = os.path.join(OUTPUT_PATH, \"evaluation_results\")\n\n# Create results directory if it doesn't exist\nos.makedirs(RESULTS_PATH, exist_ok=True)\n\ndef load_model():\n    \"\"\"Load the trained model\"\"\"\n    # Find the latest model weights\n    model_runs = [d for d in os.listdir(MODELS_PATH) if os.path.isdir(os.path.join(MODELS_PATH, d))]\n    if not model_runs:\n        print(\"No trained models found. Please train a model first.\")\n        return None\n    \n    latest_run = max(model_runs, key=lambda x: os.path.getmtime(os.path.join(MODELS_PATH, x)))\n    weights_path = os.path.join(MODELS_PATH, latest_run, \"weights\", \"best.pt\")\n    \n    if not os.path.exists(weights_path):\n        weights_path = os.path.join(MODELS_PATH, latest_run, \"weights\", \"last.pt\")\n    \n    if not os.path.exists(weights_path):\n        print(\"No model weights found. Please train a model first.\")\n        return None\n    \n    print(f\"Loading model from {weights_path}\")\n    model = YOLO(weights_path)\n    return model\n\ndef load_class_names():\n    \"\"\"Load class names from the YAML file\"\"\"\n    with open(YAML_PATH, 'r') as f:\n        config = yaml.safe_load(f)\n    return config['names']\n\ndef evaluate_model(model):\n    \"\"\"Evaluate the model on the validation dataset\"\"\"\n    if model is None:\n        return None\n    \n    print(\"Evaluating model on validation dataset...\")\n    val_path = os.path.join(OUTPUT_PATH, \"images\", \"val\")\n    \n    # Run validation\n    results = model.val(\n        data=YAML_PATH,\n        split='val',\n        batch=16,\n        imgsz=640,\n        conf=0.25,\n        iou=0.5,\n        max_det=300,\n        save_json=True,\n        save_hybrid=True,\n        plots=True\n    )\n    \n    return results\n\ndef plot_confusion_matrix(model, class_names):\n    \"\"\"Plot confusion matrix from model evaluation\"\"\"\n    conf_matrix_path = os.path.join(model.trainer.save_dir, \"confusion_matrix.png\")\n    \n    if not os.path.exists(conf_matrix_path):\n        print(\"Confusion matrix not found. Generating...\")\n        # Generate confusion matrix from model predictions\n        results = model.val(data=YAML_PATH, split='val', conf=0.25, iou=0.5)\n        \n    if os.path.exists(conf_matrix_path):\n        # Copy the confusion matrix to results directory\n        import shutil\n        shutil.copy(conf_matrix_path, os.path.join(RESULTS_PATH, \"confusion_matrix.png\"))\n        print(f\"Confusion matrix saved to {os.path.join(RESULTS_PATH, 'confusion_matrix.png')}\")\n        \n        # Display confusion matrix\n        plt.figure(figsize=(12, 10))\n        img = cv2.imread(conf_matrix_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(\"Confusion Matrix\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(RESULTS_PATH, \"confusion_matrix_display.png\"))\n        plt.show()\n    else:\n        print(\"Confusion matrix could not be generated.\")\n\ndef plot_precision_recall_curve(model):\n    \"\"\"Plot precision-recall curve from model evaluation\"\"\"\n    pr_curve_path = os.path.join(model.trainer.save_dir, \"PR_curve.png\")\n    \n    if os.path.exists(pr_curve_path):\n        # Copy the PR curve to results directory\n        import shutil\n        shutil.copy(pr_curve_path, os.path.join(RESULTS_PATH, \"PR_curve.png\"))\n        print(f\"Precision-Recall curve saved to {os.path.join(RESULTS_PATH, 'PR_curve.png')}\")\n        \n        # Display PR curve\n        plt.figure(figsize=(12, 10))\n        img = cv2.imread(pr_curve_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(\"Precision-Recall Curve\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(RESULTS_PATH, \"PR_curve_display.png\"))\n        plt.show()\n    else:\n        print(\"Precision-Recall curve not found.\")\n\ndef plot_f1_confidence_curve(model):\n    \"\"\"Plot F1-Confidence curve from model evaluation\"\"\"\n    f1_curve_path = os.path.join(model.trainer.save_dir, \"F1_curve.png\")\n    \n    if os.path.exists(f1_curve_path):\n        # Copy the F1 curve to results directory\n        import shutil\n        shutil.copy(f1_curve_path, os.path.join(RESULTS_PATH, \"F1_curve.png\"))\n        print(f\"F1-Confidence curve saved to {os.path.join(RESULTS_PATH, 'F1_curve.png')}\")\n        \n        # Display F1 curve\n        plt.figure(figsize=(12, 10))\n        img = cv2.imread(f1_curve_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(\"F1-Confidence Curve\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(RESULTS_PATH, \"F1_curve_display.png\"))\n        plt.show()\n    else:\n        print(\"F1-Confidence curve not found.\")\n\ndef calculate_performance_metrics(results):\n    \"\"\"Calculate and print performance metrics\"\"\"\n    if results is None:\n        print(\"No results available for performance metrics calculation.\")\n        return\n    \n    # Extract metrics from results\n    metrics = {\n        'mAP50': float(results.box.map50),  # Convert tensor to float\n        'mAP50-95': float(results.box.map),  # Convert tensor to float\n        'Precision': float(results.box.p.mean()),  # Mean precision\n        'Recall': float(results.box.r.mean()),  # Mean recall\n        'F1-Score': float((2 * results.box.p * results.box.r / (results.box.p + results.box.r + 1e-16)).mean())  # Mean F1-score\n    }\n    \n    # Print metrics\n    print(\"\\nPerformance Metrics:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    # Save metrics to CSV\n    metrics_df = pd.DataFrame([metrics])\n    metrics_df.to_csv(os.path.join(RESULTS_PATH, \"performance_metrics.csv\"), index=False)\n    print(f\"Performance metrics saved to {os.path.join(RESULTS_PATH, 'performance_metrics.csv')}\")\n    \n    # Plot metrics\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))\n    plt.title('Model Performance Metrics')\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    plt.savefig(os.path.join(RESULTS_PATH, \"performance_metrics.png\"))\n    plt.show()\n    \n    return metrics\n\ndef analyze_class_performance(results, class_names):\n    \"\"\"Analyze performance per class\"\"\"\n    if results is None or not hasattr(results, 'names'):\n        print(\"No results available for class performance analysis.\")\n        return\n    \n    # Extract class metrics\n    class_metrics = {}\n    for i, name in enumerate(class_names):\n        if i < len(results.box.ap50):\n            class_metrics[name] = {\n                'AP50': results.box.ap50[i].item(),\n                'Precision': results.box.p[i].item() if isinstance(results.box.p, torch.Tensor) and len(results.box.p) > i else 0,\n                'Recall': results.box.r[i].item() if isinstance(results.box.r, torch.Tensor) and len(results.box.r) > i else 0\n            }\n    \n    # Create DataFrame\n    class_df = pd.DataFrame(class_metrics).T\n    \n    # Print class metrics\n    print(\"\\nClass Performance:\")\n    print(class_df)\n    \n    # Save class metrics to CSV\n    class_df.to_csv(os.path.join(RESULTS_PATH, \"class_performance.csv\"))\n    print(f\"Class performance metrics saved to {os.path.join(RESULTS_PATH, 'class_performance.csv')}\")\n    \n    # Plot class metrics\n    plt.figure(figsize=(12, 8))\n    class_df.plot(kind='bar')\n    plt.title('Performance Metrics by Class')\n    plt.xlabel('Class')\n    plt.ylabel('Score')\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    plt.savefig(os.path.join(RESULTS_PATH, \"class_performance.png\"))\n    plt.show()\n    \n    return class_df\n\ndef main():\n    \"\"\"Main function to evaluate the model\"\"\"\n    model = load_model()\n    if model is None:\n        return\n    \n    class_names = load_class_names()\n    \n    # Evaluate model\n    results = evaluate_model(model)\n    \n    # Calculate performance metrics\n    metrics = calculate_performance_metrics(results)\n    \n    # Analyze class performance\n    class_df = analyze_class_performance(results, class_names)\n    \n    # Plot confusion matrix\n    plot_confusion_matrix(model, class_names)\n    \n    # Plot precision-recall curve\n    plot_precision_recall_curve(model)\n    \n    # Plot F1-confidence curve\n    plot_f1_confidence_curve(model)\n    print(\"\\nModel evaluation complete!\")\n    print(f\"All evaluation results saved to {RESULTS_PATH}\")\n    \n    return results, metrics, class_df\n\nif __name__ == \"__main__\":\n    results, metrics, class_df = main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference and deployment","metadata":{}},{"cell_type":"code","source":"# Inference and Deployment for YOLOv8 Model\nimport os\nimport yaml\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom PIL import Image\nimport torch\nimport time\nimport glob\nfrom tqdm import tqdm\n\n# Set the paths\nMODELS_PATH = \"/kaggle/working/models\"\nOUTPUT_PATH = \"/kaggle/working/yolo_dataset\"\nYAML_PATH = os.path.join(OUTPUT_PATH, \"visdrone.yaml\")\nINFERENCE_PATH = os.path.join(OUTPUT_PATH, \"inference_results\")\nTEST_IMAGES_PATH = os.path.join(OUTPUT_PATH, \"images\", \"test\")\nEXPORT_PATH = os.path.join(OUTPUT_PATH, \"exported_models\")\n\n# Create results directory if it doesn't exist\nos.makedirs(INFERENCE_PATH, exist_ok=True)\nos.makedirs(EXPORT_PATH, exist_ok=True)\n\ndef load_model():\n    \"\"\"Load the trained model\"\"\"\n    # Find the latest model weights\n    model_runs = [d for d in os.listdir(MODELS_PATH) if os.path.isdir(os.path.join(MODELS_PATH, d))]\n    if not model_runs:\n        print(\"No trained models found. Please train a model first.\")\n        return None\n    \n    latest_run = max(model_runs, key=lambda x: os.path.getmtime(os.path.join(MODELS_PATH, x)))\n    weights_path = os.path.join(MODELS_PATH, latest_run, \"weights\", \"best.pt\")\n    \n    if not os.path.exists(weights_path):\n        weights_path = os.path.join(MODELS_PATH, latest_run, \"weights\", \"last.pt\")\n    \n    if not os.path.exists(weights_path):\n        print(\"No model weights found. Please train a model first.\")\n        return None\n    \n    print(f\"Loading model from {weights_path}\")\n    model = YOLO(weights_path)\n    return model\n\ndef load_class_names():\n    \"\"\"Load class names from the YAML file\"\"\"\n    with open(YAML_PATH, 'r') as f:\n        config = yaml.safe_load(f)\n    return config['names']\n\ndef run_inference_on_image(model, image_path, class_names, conf_threshold=0.25, save_result=True):\n    \"\"\"Run inference on a single image\"\"\"\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Could not read image: {image_path}\")\n        return None\n    \n    # Run inference\n    start_time = time.time()\n    results = model.predict(image, conf=conf_threshold, iou=0.5, max_det=300, verbose=False)\n    inference_time = time.time() - start_time\n    \n    # Process results\n    result = results[0]\n    \n    # Get detections\n    boxes = result.boxes.xyxy.cpu().numpy()\n    scores = result.boxes.conf.cpu().numpy()\n    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n    \n    # Draw detections\n    result_image = image.copy()\n    for box, score, class_id in zip(boxes, scores, class_ids):\n        x1, y1, x2, y2 = box.astype(int)\n        label = f\"{class_names[class_id]}: {score:.2f}\"\n        \n        # Generate a color based on class_id\n        color = (int(hash(class_names[class_id]) % 255), \n                 int(hash(class_names[class_id] * 2) % 255), \n                 int(hash(class_names[class_id] * 3) % 255))\n        \n        # Draw rectangle and text\n        cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(result_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n    \n    # Save result\n    if save_result:\n        base_name = os.path.basename(image_path)\n        output_path = os.path.join(INFERENCE_PATH, f\"result_{base_name}\")\n        cv2.imwrite(output_path, result_image)\n        print(f\"Inference result saved to {output_path}\")\n    \n    return result_image, inference_time, len(boxes)\n\ndef run_inference_on_test_set(model, class_names, conf_threshold=0.25, max_images=20):\n    \"\"\"Run inference on multiple test images\"\"\"\n    print(\"Running inference on test images...\")\n    \n    # Get test images\n    test_images = glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.jpg\"))\n    if not test_images:\n        print(\"No test images found.\")\n        return\n    \n    # Limit number of images to process\n    test_images = test_images[:max_images]\n    \n    # Run inference on each image\n    inference_times = []\n    detection_counts = []\n    \n    for img_path in tqdm(test_images):\n        result_image, inference_time, detection_count = run_inference_on_image(\n            model, img_path, class_names, conf_threshold\n        )\n        inference_times.append(inference_time)\n        detection_counts.append(detection_count)\n    \n    # Calculate statistics\n    avg_time = np.mean(inference_times)\n    avg_detections = np.mean(detection_counts)\n    \n    print(f\"\\nInference on {len(test_images)} test images complete.\")\n    print(f\"Average inference time: {avg_time:.4f} seconds per image\")\n    print(f\"Average number of detections: {avg_detections:.2f}\")\n    \n    # Create inference report\n    report = {\n        \"num_images\": len(test_images),\n        \"avg_inference_time\": avg_time,\n        \"avg_detections\": avg_detections,\n        \"confidence_threshold\": conf_threshold\n    }\n    \n    # Save report\n    import json\n    with open(os.path.join(INFERENCE_PATH, \"inference_report.json\"), \"w\") as f:\n        json.dump(report, f, indent=4)\n    \n    return report\n\ndef run_inference_from_webcam(model, class_names, conf_threshold=0.25):\n    \"\"\"Run inference on webcam (for demonstration purposes)\"\"\"\n    print(\"Starting webcam inference...\")\n    \n    # Check if webcam exists\n    cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        print(\"Could not open webcam. Skipping webcam inference.\")\n        return\n    \n    # Set up window\n    cv2.namedWindow(\"YOLOv8 Inference\", cv2.WINDOW_NORMAL)\n    \n    try:\n        while True:\n            # Read frame\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            # Run inference\n            results = model.predict(frame, conf=conf_threshold, iou=0.5, max_det=300, verbose=False)\n            result = results[0]\n            \n            # Get detections\n            boxes = result.boxes.xyxy.cpu().numpy()\n            scores = result.boxes.conf.cpu().numpy()\n            class_ids = result.boxes.cls.cpu().numpy().astype(int)\n            \n            # Draw detections\n            for box, score, class_id in zip(boxes, scores, class_ids):\n                x1, y1, x2, y2 = box.astype(int)\n                label = f\"{class_names[class_id]}: {score:.2f}\"\n                \n                # Generate a color based on class_id\n                color = (int(hash(class_names[class_id]) % 255), \n                        int(hash(class_names[class_id] * 2) % 255), \n                        int(hash(class_names[class_id] * 3) % 255))\n                \n                # Draw rectangle and text\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n            \n            # Show frame\n            cv2.imshow(\"YOLOv8 Inference\", frame)\n            \n            # Break loop if 'q' pressed\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n    finally:\n        # Release resources\n        cap.release()\n        cv2.destroyAllWindows()\n\ndef export_model(model, format=\"onnx\"):\n    \"\"\"Export the model to the specified format\"\"\"\n    print(f\"Exporting model to {format} format...\")\n    \n    # Supported formats\n    supported_formats = [\"onnx\", \"torchscript\", \"openvino\", \"coreml\", \"tflite\"]\n    \n    if format not in supported_formats:\n        print(f\"Unsupported format: {format}. Supported formats: {supported_formats}\")\n        return\n    \n    # Export model\n    try:\n        model.export(format=format, imgsz=640)\n        \n        # Move exported model to EXPORT_PATH\n        source_path = os.path.join(os.path.dirname(model.ckpt_path), f\"{os.path.basename(model.ckpt_path).split('.')[0]}.{format}\")\n        if os.path.exists(source_path):\n            import shutil\n            dest_path = os.path.join(EXPORT_PATH, f\"yolov8_visdrone.{format}\")\n            shutil.copy(source_path, dest_path)\n            print(f\"Model exported to {dest_path}\")\n    except Exception as e:\n        print(f\"Error exporting model: {e}\")\n\ndef export_all_formats(model):\n    \"\"\"Export the model to all supported formats\"\"\"\n    formats = [\"onnx\", \"torchscript\", \"openvino\", \"tflite\"]\n    for format in formats:\n        export_model(model, format)\n\ndef main():\n    \"\"\"Main function for inference and deployment\"\"\"\n    model = load_model()\n    if model is None:\n        return\n    \n    class_names = load_class_names()\n    \n    # Run inference on test set\n    report = run_inference_on_test_set(model, class_names, conf_threshold=0.25, max_images=20)\n    \n    # Export model\n    while True:\n        print(\"\\nChoose model export format:\")\n        print(\"1. ONNX (recommended for deployment)\")\n        print(\"2. TorchScript\")\n        print(\"3. OpenVINO\")\n        print(\"4. TensorFlow Lite\")\n        print(\"5. Export all formats\")\n        print(\"6. Skip export\")\n        \n        choice = input(\"Enter your choice (default: 1): \") or \"1\"\n        \n        if choice == \"1\":\n            export_model(model, \"onnx\")\n            break\n        elif choice == \"2\":\n            export_model(model, \"torchscript\")\n            break\n        elif choice == \"3\":\n            export_model(model, \"openvino\")\n            break\n        elif choice == \"4\":\n            export_model(model, \"tflite\")\n            break\n        elif choice == \"5\":\n            export_all_formats(model)\n            break\n        elif choice == \"6\":\n            print(\"Skipping model export.\")\n            break\n        else:\n            print(\"Invalid choice. Please try again.\")\n    \n    print(\"\\nInference and deployment complete!\")\n    print(f\"All inference results saved to {INFERENCE_PATH}\")\n    print(f\"Exported models saved to {EXPORT_PATH}\")\n    \n    return model, report\n\nif __name__ == \"__main__\":\n    model, report = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:47:23.909981Z","iopub.execute_input":"2025-03-19T10:47:23.910305Z","iopub.status.idle":"2025-03-19T10:47:46.444134Z","shell.execute_reply.started":"2025-03-19T10:47:23.910280Z","shell.execute_reply":"2025-03-19T10:47:46.443198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nfrom glob import glob\nfrom PIL import Image\n\n# Path to the folder containing images\nimage_folder = \"/kaggle/working/yolo_dataset/inference_results/\"\n\n# Get all image file paths\nimage_paths = sorted(glob(os.path.join(image_folder, \"*.jpg\")))  # Change \"*.jpg\" if images have a different extension\n\n# Plot images\nplt.figure(figsize=(15, 15))\ncolumns = 4\nrows = (len(image_paths) + columns - 1) // columns  # Calculate number of rows dynamically\n\nfor i, image_path in enumerate(image_paths, 1):\n    img = Image.open(image_path)\n    plt.subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(os.path.basename(image_path))\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:47:46.445349Z","iopub.execute_input":"2025-03-19T10:47:46.445717Z","iopub.status.idle":"2025-03-19T10:47:51.676941Z","shell.execute_reply.started":"2025-03-19T10:47:46.445683Z","shell.execute_reply":"2025-03-19T10:47:51.675525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DeepSort","metadata":{}},{"cell_type":"code","source":"pip install ultralytics torch opencv-python numpy tqdm supervision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T11:00:06.594231Z","iopub.execute_input":"2025-03-19T11:00:06.594644Z","iopub.status.idle":"2025-03-19T11:00:10.732436Z","shell.execute_reply.started":"2025-03-19T11:00:06.594614Z","shell.execute_reply":"2025-03-19T11:00:10.731295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install lap","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nfrom ultralytics import YOLO\nfrom tqdm import tqdm\nimport lap  # Linear assignment problem solver\n\nclass KalmanFilter:\n    \"\"\"Simple Kalman Filter implementation for tracking\"\"\"\n    def __init__(self):\n        # State transition matrix\n        self.F = np.array([[1, 0, 0, 0, 1, 0, 0, 0],\n                           [0, 1, 0, 0, 0, 1, 0, 0],\n                           [0, 0, 1, 0, 0, 0, 1, 0],\n                           [0, 0, 0, 1, 0, 0, 0, 1],\n                           [0, 0, 0, 0, 1, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 1, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 1, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 1]])\n        \n        # Measurement matrix\n        self.H = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 1, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 1, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 1, 0, 0, 0, 0]])\n        \n        # Measurement noise\n        self.R = np.eye(4) * 0.1\n        \n        # Process noise\n        self.Q = np.eye(8) * 0.1\n        self.Q[4:, 4:] *= 10\n        \n        # Error covariance\n        self.P = np.eye(8) * 10\n        \n        # State\n        self.x = np.zeros((8, 1))\n        \n    def predict(self):\n        \"\"\"Predict next state\"\"\"\n        self.x = self.F @ self.x\n        self.P = self.F @ self.P @ self.F.T + self.Q\n        return self.x[:4].flatten()\n        \n    def update(self, z):\n        \"\"\"Update state with measurement z\"\"\"\n        z = z.reshape(-1, 1)\n        y = z - self.H @ self.x\n        S = self.H @ self.P @ self.H.T + self.R\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n        self.x = self.x + K @ y\n        I = np.eye(8)\n        self.P = (I - K @ self.H) @ self.P\n        return self.x[:4].flatten()\n        \n    def initiate(self, measurement):\n        \"\"\"Initialize state with first measurement\"\"\"\n        self.x = np.zeros((8, 1))\n        self.x[:4, 0] = measurement\n        return self.x[:4].flatten()\n\n\nclass Track:\n    \"\"\"Track class for DeepSORT\"\"\"\n    count = 0\n    \n    def __init__(self, detection, class_id):\n        self.id = Track.count\n        Track.count += 1\n        \n        self.class_id = class_id\n        self.hits = 1\n        self.age = 1\n        self.time_since_update = 0\n        \n        # Initialize Kalman filter\n        self.kf = KalmanFilter()\n        \n        # Initialize state\n        box = detection[:4]\n        self.mean = self.kf.initiate(box)\n        \n        # Detection confidence\n        self.confidence = detection[4] if len(detection) > 4 else 1.0\n        \n    def predict(self):\n        \"\"\"Predict next state\"\"\"\n        self.mean = self.kf.predict()\n        self.age += 1\n        self.time_since_update += 1\n        return self.mean\n        \n    def update(self, detection):\n        \"\"\"Update track with new detection\"\"\"\n        box = detection[:4]\n        self.mean = self.kf.update(box)\n        self.hits += 1\n        self.time_since_update = 0\n        \n        # Update confidence\n        if len(detection) > 4:\n            self.confidence = detection[4]\n        \n        # Update class_id if provided\n        if len(detection) > 5:\n            self.class_id = detection[5]\n        \n        return self.mean\n        \n    def get_state(self):\n        \"\"\"Get current state\"\"\"\n        return self.mean\n    \n    def is_confirmed(self):\n        \"\"\"Check if track is confirmed\"\"\"\n        return self.hits >= 3\n    \n    def is_deleted(self):\n        \"\"\"Check if track should be deleted\"\"\"\n        return self.time_since_update > 30\n\n\nclass SimpleTracker:\n    \"\"\"Simple implementation of DeepSORT tracker\"\"\"\n    def __init__(self):\n        self.tracks = []\n        self.max_iou_distance = 0.7\n        self.max_age = 30\n        self.min_hits = 3\n        \n    def update(self, detections):\n        \"\"\"Update tracks with new detections\"\"\"\n        # Predict locations of existing tracks\n        for track in self.tracks:\n            track.predict()\n        \n        # Match detections to tracks\n        matches, unmatched_tracks, unmatched_detections = self._match(detections)\n        \n        # Update matched tracks\n        for track_idx, detection_idx in matches:\n            self.tracks[track_idx].update(detections[detection_idx])\n        \n        # Mark unmatched tracks\n        for track_idx in unmatched_tracks:\n            self.tracks[track_idx].time_since_update += 1\n        \n        # Add new tracks\n        for detection_idx in unmatched_detections:\n            detection = detections[detection_idx]\n            class_id = int(detection[5]) if len(detection) > 5 else -1\n            self.tracks.append(Track(detection, class_id))\n        \n        # Remove dead tracks\n        self.tracks = [t for t in self.tracks if not t.is_deleted()]\n        \n        # Get results\n        results = []\n        for track in self.tracks:\n            if track.is_confirmed():\n                box = track.get_state()\n                results.append(np.append(box, [track.id, track.class_id, track.confidence]))\n        \n        return np.array(results) if results else np.empty((0, 7))\n    \n    def _match(self, detections):\n        \"\"\"Match detections to tracks using IoU\"\"\"\n        if len(self.tracks) == 0 or len(detections) == 0:\n            return [], list(range(len(self.tracks))), list(range(len(detections)))\n        \n        # Compute IoU matrix\n        iou_matrix = np.zeros((len(self.tracks), len(detections)))\n        for i, track in enumerate(self.tracks):\n            for j, detection in enumerate(detections):\n                iou_matrix[i, j] = self._iou(track.get_state(), detection[:4])\n        \n        # Convert to cost matrix\n        cost_matrix = 1 - iou_matrix\n        \n        # Solve linear assignment problem\n        row_indices, col_indices = lap.lapjv(cost_matrix, extend_cost=True)[0:2]\n        \n        # Filter matches\n        matches = []\n        unmatched_tracks = []\n        unmatched_detections = []\n        \n        for track_idx, detection_idx in enumerate(col_indices):\n            if detection_idx >= 0:\n                if cost_matrix[track_idx, detection_idx] > self.max_iou_distance:\n                    unmatched_tracks.append(track_idx)\n                    unmatched_detections.append(detection_idx)\n                else:\n                    matches.append((track_idx, detection_idx))\n            else:\n                unmatched_tracks.append(track_idx)\n        \n        # Add unmatched detections\n        for detection_idx in range(len(detections)):\n            if detection_idx not in col_indices:\n                unmatched_detections.append(detection_idx)\n        \n        return matches, unmatched_tracks, unmatched_detections\n    \n    def _iou(self, box1, box2):\n        \"\"\"Calculate IoU between two boxes\"\"\"\n        # Convert to [x1, y1, x2, y2] if necessary\n        box1 = np.array(box1).reshape(-1)\n        box2 = np.array(box2).reshape(-1)\n        \n        # Calculate intersection area\n        x1 = max(box1[0], box2[0])\n        y1 = max(box1[1], box2[1])\n        x2 = min(box1[2], box2[2])\n        y2 = min(box1[3], box2[3])\n        \n        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n        \n        # Calculate union area\n        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n        \n        union = area1 + area2 - intersection\n        \n        # Calculate IoU\n        return intersection / union if union > 0 else 0\n\n\nclass ObjectTracker:\n    \"\"\"Object tracker using YOLOv8 and Simple DeepSORT\"\"\"\n    def __init__(self, model_path):\n        \"\"\"Initialize tracker with YOLOv8 model\"\"\"\n        # Load YOLOv8 model\n        self.model = YOLO(model_path)\n        \n        # Initialize tracker\n        self.tracker = SimpleTracker()\n        \n        # Class names for VisDrone dataset\n        self.class_names = ['pedestrian', 'people', 'bicycle', 'car', 'van', \n                           'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor']\n        \n        # Dictionary to store trajectory points for each object\n        self.trajectories = {}\n        \n        # Dictionary to store unique colors for each track ID\n        self.track_colors = {}\n    \n    def _get_color_for_track(self, track_id):\n        \"\"\"Get a unique color for a track ID\"\"\"\n        if track_id not in self.track_colors:\n            # Generate a random color that's not too dark\n            color = np.random.randint(80, 255, size=3).tolist()\n            self.track_colors[track_id] = color\n        return self.track_colors[track_id]\n    \n    def process_sequence(self, sequence_path, output_path, conf_threshold=0.3):\n        \"\"\"Process image sequence and create tracking video\"\"\"\n        # Get all image files\n        image_files = sorted([f for f in os.listdir(sequence_path) \n                             if f.endswith(('.jpg', '.png', '.jpeg'))])\n        \n        if not image_files:\n            print(f\"No images found in {sequence_path}\")\n            return\n        \n        # Read first image to get dimensions\n        first_image = cv2.imread(os.path.join(sequence_path, image_files[0]))\n        height, width = first_image.shape[:2]\n        \n        # Create output directory\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        \n        # Initialize video writer\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))\n        \n        # Process each frame\n        for img_file in tqdm(image_files, desc=\"Processing frames\"):\n            # Read image\n            img_path = os.path.join(sequence_path, img_file)\n            frame = cv2.imread(img_path)\n            if frame is None:\n                continue\n            \n            # Run YOLOv8 detection\n            results = self.model(frame, conf=conf_threshold, verbose=False)\n            \n            # Prepare detections for tracker\n            detections = []\n            if results[0].boxes.data.shape[0] > 0:\n                # Get detection data: [x1, y1, x2, y2, conf, class_id]\n                boxes = results[0].boxes.data.cpu().numpy()\n                \n                # Format detections for tracker\n                for box in boxes:\n                    x1, y1, x2, y2 = box[:4]\n                    conf = box[4]\n                    class_id = int(box[5]) if len(box) > 5 else 0\n                    detections.append([x1, y1, x2, y2, conf, class_id])\n            \n            # Update tracker\n            if detections:\n                tracks = self.tracker.update(np.array(detections))\n                \n                # First draw trajectories\n                for track in tracks:\n                    # Extract tracking info\n                    x1, y1, x2, y2, track_id, class_id, conf = track\n                    \n                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n                    track_id = int(track_id)\n                    \n                    # Calculate center point of bounding box\n                    center_x = int((x1 + x2) / 2)\n                    center_y = int((y1 + y2) / 2)\n                    \n                    # Get unique color for this track ID\n                    color = self._get_color_for_track(track_id)\n                    \n                    # Add center point to trajectory\n                    if track_id not in self.trajectories:\n                        self.trajectories[track_id] = []\n                    self.trajectories[track_id].append((center_x, center_y))\n                    \n                    # Draw trajectory line\n                    if len(self.trajectories[track_id]) > 1:\n                        points = np.array(self.trajectories[track_id], np.int32)\n                        cv2.polylines(frame, [points], False, color, 2)\n                \n                # Then draw bounding boxes (so they're on top of lines)\n                for track in tracks:\n                    # Extract tracking info\n                    x1, y1, x2, y2, track_id, class_id, conf = track\n                    \n                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n                    class_id = int(class_id)\n                    track_id = int(track_id)\n                    \n                    # Get unique color for this track ID\n                    color = self._get_color_for_track(track_id)\n                    \n                    # Get class name\n                    class_name = self.class_names[class_id] if class_id < len(self.class_names) else 'unknown'\n                    \n                    # Draw bounding box\n                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                    \n                    # Draw class name and track ID\n                    label = f\"{class_name}-{track_id}\"\n                    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]\n                    cv2.rectangle(frame, (x1, y1-t_size[1]-10), (x1+t_size[0], y1), color, -1)\n                    cv2.putText(frame, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n            \n            # Write frame to output video\n            out.write(frame)\n            \n            # Optional: Save each processed frame as an image\n            output_img_dir = os.path.join(os.path.dirname(output_path), \"processed_frames\")\n            os.makedirs(output_img_dir, exist_ok=True)\n            cv2.imwrite(os.path.join(output_img_dir, img_file), frame)\n        \n        # Release video writer\n        out.release()\n        print(f\"Output saved to {output_path}\")\n        print(f\"Processed frames saved to {output_img_dir}\")\n        \n        # Return output path\n        return output_path\n\n    def clear_trajectories(self):\n        \"\"\"Clear all tracked trajectories\"\"\"\n        self.trajectories = {}\n        self.track_colors = {}\n\n\n# Function to process a test sequence\ndef process_test_sequence(model_path, sequence_path):\n    \"\"\"Process a test sequence with tracking\"\"\"\n    # Initialize tracker\n    tracker = ObjectTracker(model_path)\n    \n    # Output path\n    os.makedirs(\"tracking_results\", exist_ok=True)\n    sequence_name = os.path.basename(sequence_path)\n    output_path = f\"tracking_results/{sequence_name}_tracked.mp4\"\n    \n    # Process sequence\n    result_path = tracker.process_sequence(sequence_path, output_path)\n    \n    print(f\"Tracking completed! Video saved to: {result_path}\")\n    return result_path\n\n\n# Main function\ndef main():\n    # Path to your YOLOv8 model\n    model_path = \"yolov8n.pt\"  # Update this with your model path\n    \n    # Direct path to the sequence folder\n    sequence_path = \"/kaggle/input/cv-multiobject-dectection-dataset/VisDrone2019-MOT-train/VisDrone2019-MOT-train/sequences/uav0000013_00000_v\"\n    \n    # Process the sequence\n    result_path = process_test_sequence(model_path, sequence_path)\n    \n    # Display information about the result\n    print(f\"Tracking results saved to: {result_path}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T11:26:10.071708Z","iopub.execute_input":"2025-03-19T11:26:10.072085Z","iopub.status.idle":"2025-03-19T11:26:19.486385Z","shell.execute_reply.started":"2025-03-19T11:26:10.072059Z","shell.execute_reply":"2025-03-19T11:26:19.485383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}